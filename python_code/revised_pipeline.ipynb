{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:27.512294Z",
     "start_time": "2025-07-01T08:25:27.505553Z"
    }
   },
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from textstat import textstat\n",
    "import re\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "\n",
    "# Import our parallel processing utilities\n",
    "from parallel import (\n",
    "    process_batch_with_progress,\n",
    "    process_dataframe_parallel,\n",
    "    gpu_batch_process,\n",
    "    resilient_api_call,\n",
    "    CheckpointManager\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:28.365987Z",
     "start_time": "2025-07-01T08:25:27.546539Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = spacy.load(\"en_core_web_sm\")\n",
   "id": "e5ebf3e02cef4812",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:28.407439Z",
     "start_time": "2025-07-01T08:25:28.382032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Should not be None\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "74430367fa940bc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:31.009464Z",
     "start_time": "2025-07-01T08:25:28.425844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda available!\")\n",
    "    model.to(\"cuda\")\n",
    "else:\n",
    "    print('cuda not available!')\n"
   ],
   "id": "97314cfed896534a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## fetch article titles",
   "id": "e02e9814e1a0885d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:31.056297Z",
     "start_time": "2025-07-01T08:25:31.045665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_category_members(category, cmtype=\"page\", namespace=0, limit=500):\n",
    "    \"\"\"\n",
    "    Fetches members of a given Wikipedia category with improved error handling.\n",
    "\n",
    "    Args:\n",
    "        category: Category name without the 'Category:' prefix\n",
    "        cmtype: 'page', 'subcat', or 'file'\n",
    "        namespace: Namespace index (0 for articles)\n",
    "        limit: Number of results per request (max 500 for users)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'pageid' and 'title'\n",
    "    \"\"\"\n",
    "\n",
    "    def api_call(**params):\n",
    "        S = requests.Session()\n",
    "        res = S.get(\"https://en.wikipedia.org/w/api.php\", params=params, timeout=30)\n",
    "        res.raise_for_status()\n",
    "        return res.json()\n",
    "\n",
    "    members = []\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": f\"Category:{category}\",\n",
    "        \"cmtype\": cmtype,\n",
    "        \"cmlimit\": limit,\n",
    "        \"cmnamespace\": namespace,\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Use our resilient API call function with retries\n",
    "            data = resilient_api_call(api_call, max_retries=5, **params)\n",
    "            batch = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "            members.extend(batch)\n",
    "\n",
    "            if \"continue\" in data:\n",
    "                params.update(data[\"continue\"])\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching category members for {category}: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    return members\n"
   ],
   "id": "cb48f4e4520c558e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:31.111856Z",
     "start_time": "2025-07-01T08:25:31.103817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_articles_from_categories(categories, include_subcats=False, max_subcat_depth=1):\n",
    "    \"\"\"\n",
    "    Given a list of category names, fetches all articles in them.\n",
    "    Optionally includes pages from subcategories up to specified depth.\n",
    "\n",
    "    :param categories: List of category names (strings without prefix)\n",
    "    :param include_subcats: Whether to traverse into subcategories\n",
    "    :param max_subcat_depth: Maximum depth for subcategory traversal\n",
    "    :returns: Set of page titles\n",
    "    \"\"\"\n",
    "    all_articles = set()\n",
    "    seen_cats = set()\n",
    "\n",
    "    def _recurse(cat, depth):\n",
    "        if cat in seen_cats or depth < 0:\n",
    "            return\n",
    "        seen_cats.add(cat)\n",
    "\n",
    "        # Fetch pages\n",
    "        pages = get_category_members(cat, cmtype=\"page\")\n",
    "        for p in pages:\n",
    "            all_articles.add(p[\"title\"])\n",
    "\n",
    "        if include_subcats and depth > 0:\n",
    "            subcats = get_category_members(cat, cmtype=\"subcat\", namespace=14)\n",
    "            for sc in subcats:\n",
    "                sc_name = sc[\"title\"].replace(\"Category:\", \"\")\n",
    "                _recurse(sc_name, depth - 1)\n",
    "\n",
    "    for cat in tqdm(categories, desc=\"Traversing categories\"):\n",
    "        _recurse(cat, max_subcat_depth)\n",
    "\n",
    "    return all_articles\n"
   ],
   "id": "38735abec82ab494",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Declare root categories and their subcategories",
   "id": "dc6cfa6fcb008c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:25:31.166375Z",
     "start_time": "2025-07-01T08:25:31.159346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root_categories = {\n",
    "    \"Politics\": [\n",
    "        \"Politics\", \"Political history\", \"Elections\", \"Political parties\"\n",
    "    ],\n",
    "    \"Science & Medicine\": [\n",
    "        \"Science\", \"Medicine\", \"Biology\", \"Physics\", \"Chemistry\"\n",
    "    ],\n",
    "    \"History\": [\n",
    "        \"History\", \"Military history\", \"History by country\"\n",
    "    ],\n",
    "    \"Technology\": [\n",
    "        \"Technology\", \"Computing\", \"Engineering\"\n",
    "    ],\n",
    "    \"Popular Culture\": [\n",
    "        \"Popular culture\", \"Music\", \"Television\", \"Film\", \"Video games\"\n",
    "    ]\n",
    "}\n"
   ],
   "id": "a341d9264dc157a8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Traverse root categories and fetch articles",
   "id": "545cbac5dd372a73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:28:13.030784Z",
     "start_time": "2025-07-01T08:25:31.204093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name_articles_list = \"articles_by_category_1_recurse.csv\"\n",
    "all_records = []\n",
    "for stratum, roots in root_categories.items():\n",
    "    for root in roots:\n",
    "        articles = fetch_articles_from_categories([root], include_subcats=True, max_subcat_depth=1)\n",
    "        for title in articles:\n",
    "            all_records.append({\"stratum\": stratum, \"root\": root, \"title\": title})\n",
    "pd.DataFrame(all_records).to_csv(file_name_articles_list, index=False)\n"
   ],
   "id": "1761bee7b1d4bfa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac585bf048f8474ebaef5f73765355a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20434be3896744668980d0957566f33d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "beece4f01cf7466abb469d3df0a701a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ce5866fd70249f3b4d2d3b63835e4cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46e9939fc5ec42039ead3c25e392c74e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8e063a656124cb599de0d531e782da1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68eedea3443744789c671546dec3086e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f32db2e22aa4251a74935150eba4003"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11d6d07bd2ff4719ad45f64c32bea7a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9527451d868846d1877bef262e12594f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29f9f4ba84764e309cde78d1df5d19e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea5b5b14edc04bb48f35eff451f8b83e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbe2fe310a6f46b9b9b7a3aed715093b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8de0271a898c4cfd97c9b8bba011c270"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdce2efb25774f99b7107f50a7e0ce02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53926fa61d474bd390a33e2854776ff2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5003670b0c6249e4a58ddeae9c90609c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb657cd0ade648d3a670385f408c94c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30874ac3f95446e684f4d65f92ad9816"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a36ebff58c949e499103da406f7c9cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:28:13.112469Z",
     "start_time": "2025-07-01T08:28:13.105707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check fetched articles\n",
    "# print len of all_records including subcats\n",
    "len(all_records)"
   ],
   "id": "d8272ebe29f76b56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28713"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:34:00.552474Z",
     "start_time": "2025-07-01T08:34:00.500921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the articles from the saved file\n",
    "article = pd.read_csv(file_name_articles_list)"
   ],
   "id": "5d2f5be038536e0e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T08:34:08.481454Z",
     "start_time": "2025-07-01T08:34:08.475991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_bot_username(username: str) -> bool:\n",
    "    return username.lower().endswith(\"bot\")"
   ],
   "id": "fece11c26267e6d1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fetch_revision_snapshots(\n",
    "        title: str,\n",
    "        start_ts: str,\n",
    "        end_ts: str,\n",
    "        freq: str = \"7D\",\n",
    "        bot_test_fn: callable = None,\n",
    "        carry_forward: bool = True,\n",
    "        max_retries: int = 5,\n",
    "        backoff_factor: float = 1.0,\n",
    "        checkpoint_path: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches all revisions (including full content) for `title` between start_ts and end_ts,\n",
    "    then returns snapshots at given frequency with content included.\n",
    "\n",
    "    This version includes improved error handling and checkpointing.\n",
    "\n",
    "    Args:\n",
    "        title: Wikipedia page title\n",
    "        start_ts: ISO8601 timestamp string, e.g. \"2020-01-01T00:00:00Z\"\n",
    "        end_ts: ISO8601 timestamp string, e.g. \"2020-12-31T23:59:59Z\"\n",
    "        freq: pandas offset alias (e.g. \"7D\")\n",
    "        bot_test_fn: callable to flag bots (user->bool)\n",
    "        carry_forward: reuse last snapshot if no newer revision\n",
    "        max_retries: number of retries on network failure\n",
    "        backoff_factor: multiplier for retry backoff in seconds\n",
    "        checkpoint_path: path to pickle intermediate results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns ['page_title','snapshot_ts','rev_id','timestamp',\n",
    "        'user','is_bot','content']\n",
    "    \"\"\"\n",
    "    # Check if we have a checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            return pd.read_pickle(checkpoint_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "\n",
    "    def api_call(**params):\n",
    "        session = requests.Session()\n",
    "        resp = session.get(\"https://en.wikipedia.org/w/api.php\", params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    # Phase 1: Fetch metadata + content in one pass\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"ids|timestamp|user|content\",\n",
    "        \"rvstart\": end_ts,\n",
    "        \"rvend\": start_ts,\n",
    "        \"rvlimit\": \"max\",\n",
    "        \"titles\": title,\n",
    "        \"redirects\": 1,\n",
    "        \"rvslots\": \"main\",\n",
    "    }\n",
    "\n",
    "    all_revs = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Use our resilient API call function\n",
    "            data = resilient_api_call(\n",
    "                api_call,\n",
    "                max_retries=max_retries,\n",
    "                initial_backoff=backoff_factor,\n",
    "                **params\n",
    "            )\n",
    "\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            for page in pages.values():\n",
    "                for rev in page.get(\"revisions\", []) or []:\n",
    "                    ts_str = rev.get(\"timestamp\")\n",
    "                    if not ts_str:\n",
    "                        continue\n",
    "                    ts = parser.isoparse(ts_str)\n",
    "                    user = rev.get(\"user\", \"\")\n",
    "                    # content slot\n",
    "                    content = rev.get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
    "                    all_revs.append({\n",
    "                        \"rev_id\": rev.get(\"revid\"),\n",
    "                        \"timestamp\": ts,\n",
    "                        \"user\": user,\n",
    "                        \"is_bot\": bot_test_fn(user) if bot_test_fn else False,\n",
    "                        \"content\": content,\n",
    "                    })\n",
    "\n",
    "            if \"continue\" in data:\n",
    "                params.update(data[\"continue\"])\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching revisions for {title}: {str(e)}\")\n",
    "        # If we have some revisions, continue with what we have\n",
    "        if not all_revs:\n",
    "            return pd.DataFrame(columns=[\"page_title\", \"snapshot_ts\", \"rev_id\",\n",
    "                                         \"timestamp\", \"user\", \"is_bot\", \"content\"])\n",
    "\n",
    "    # If no revisions, return empty DataFrame\n",
    "    if not all_revs:\n",
    "        return pd.DataFrame(columns=[\"page_title\", \"snapshot_ts\", \"rev_id\",\n",
    "                                     \"timestamp\", \"user\", \"is_bot\", \"content\"])\n",
    "\n",
    "    # Build DataFrame and sort\n",
    "    df = pd.DataFrame(all_revs)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Phase 2: Snapshot selection\n",
    "    timestamps = df[\"timestamp\"]\n",
    "    sample_times = pd.date_range(\n",
    "        start=pd.to_datetime(start_ts),\n",
    "        end=pd.to_datetime(end_ts),\n",
    "        freq=freq,\n",
    "        tz=timestamps.dt.tz\n",
    "    )\n",
    "\n",
    "    snaps = []\n",
    "    last_snap_time = None\n",
    "\n",
    "    for snap_t in sample_times:\n",
    "        pos = timestamps.searchsorted(snap_t, side='right') - 1\n",
    "        if pos < 0:\n",
    "            continue\n",
    "        rev_time = timestamps.iloc[pos]\n",
    "        # skip if no carry_forward and no new revision\n",
    "        if not carry_forward and last_snap_time is not None and rev_time <= last_snap_time:\n",
    "            last_snap_time = snap_t\n",
    "            continue\n",
    "        row = df.iloc[pos].to_dict()\n",
    "        row[\"snapshot_ts\"] = snap_t\n",
    "        row[\"page_title\"] = title\n",
    "        snaps.append(row)\n",
    "        last_snap_time = snap_t\n",
    "\n",
    "    df_snap = pd.DataFrame(snaps)\n",
    "    cols = [\"page_title\", \"snapshot_ts\", \"rev_id\", \"timestamp\", \"user\", \"is_bot\", \"content\"]\n",
    "    result_df = df_snap[cols] if not df_snap.empty else pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Save checkpoint if path provided\n",
    "    if checkpoint_path and not result_df.empty:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            result_df.to_pickle(checkpoint_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "    return result_df\n"
   ],
   "id": "6bceebda54276ffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modify parallel processing configuration\n",
    "from parallel import process_batch_with_progress\n",
    "\n",
    "# Define categories to fetch\n",
    "categories = [\n",
    "    \"Politics\",\n",
    "    \"History\"\n",
    "]\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Configure more conservative parallel processing settings\n",
    "PARALLEL_CONFIG = {\n",
    "    'max_workers': 4,  # Reduce number of concurrent workers\n",
    "    'use_threads': True,  # Use threads instead of processes for API calls\n",
    "    'cpu_intensive': False,\n",
    "    'timeout': 300,  # 5 minutes timeout\n",
    "    'chunk_size': 5  # Process in smaller chunks\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch articles with improved error handling\n",
    "def fetch_with_retries(category, **kwargs):\n",
    "    try:\n",
    "        results = fetch_articles_from_categories(\n",
    "            [category],\n",
    "            include_subcats=True,\n",
    "            max_subcat_depth=2\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing category {category}: {str(e)}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "# Process categories with more robust error handling\n",
    "articles = set()\n",
    "results = process_batch_with_progress(\n",
    "    fetch_with_retries,\n",
    "    categories,\n",
    "    desc=\"Processing categories\",\n",
    "    **PARALLEL_CONFIG\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "for result in results:\n",
    "    if result:\n",
    "        articles.update(result)\n",
    "\n",
    "print(f\"Fetched {len(articles)} articles from categories and subcategories.\")\n",
    "\n",
    "# Save to file with error handling\n",
    "try:\n",
    "    with open(\"FINAL_NO_RECURSE_article_names_list.pkl\", \"wb\") as f:\n",
    "        pickle.dump(articles, f)\n",
    "    print(\"Successfully saved articles list\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving articles list: {str(e)}\")\n"
   ],
   "id": "d0db2d5f448c3db6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:09:12.724604Z",
     "start_time": "2025-07-01T09:09:12.707200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load the articles from the saved file if needed\n",
    "# try:\n",
    "#     with open(\"article_names_list.pkl\", \"rb\") as f:\n",
    "#         articles = pickle.load(f)\n",
    "#     print(f\"Loaded {len(articles)} articles from file.\")\n",
    "#     print(articles.pop())  # Print one article to verify\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Article list file not found.\")\n",
    "#\n",
    "#\n",
    "\n"
   ],
   "id": "588ffbee50ec66b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29306 articles from file.\n",
      "Pregnancy school\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T09:08:48.971659Z",
     "start_time": "2025-07-01T09:08:48.917549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load from all_articles_by_category.csv if available\n",
    "try:\n",
    "    articles_df = pd.read_csv(\"all_articles_by_category.csv\")\n",
    "    articles = set(articles_df[\"title\"].tolist())\n",
    "    print(f\"Loaded {len(articles)} articles from CSV.\")\n",
    "\n",
    "    print(articles_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Using previously fetched articles.\")\n"
   ],
   "id": "1c942ef31a06f94b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27725 articles from CSV.\n",
      "    stratum      root                                  title\n",
      "0  Politics  Politics                   Modernization theory\n",
      "1  Politics  Politics  Timeline of incidents involving QAnon\n",
      "2  Politics  Politics                 Crises of the Republic\n",
      "3  Politics  Politics          Governance of protected areas\n",
      "4  Politics  Politics                    Political ReviewNet\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
