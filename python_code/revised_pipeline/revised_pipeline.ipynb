{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch article titles from root categories and their subcategories",
   "id": "ec42155fbbc04ee1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:32:13.858036Z",
     "start_time": "2025-07-01T17:32:13.849648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fetch_titles, fetch_revisions\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import parallel\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ],
   "id": "f9542147462dabba",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:49:35.859032Z",
     "start_time": "2025-07-01T12:49:35.853305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "# Map of supercategories to lists of root categories (without 'Category:' prefix)\n",
    "ROOT_CATEGORIES: Dict[str, List[str]] = {\n",
    "    \"Politics\": [\n",
    "        \"Politics\"  #, \"Political history\", \"Elections\", \"Political parties\"\n",
    "        # ],\n",
    "        # \"Science & Medicine\": [\n",
    "        #     \"Science\", \"Medicine\", \"Biology\", \"Physics\", \"Chemistry\"\n",
    "        # ],\n",
    "        # \"History\": [\n",
    "        #     \"History\", \"Military history\", \"History by country\"\n",
    "        # ],\n",
    "        # \"Technology\": [\n",
    "        #     \"Technology\", \"Computing\", \"Engineering\"\n",
    "        # ],\n",
    "        # \"Popular Culture\": [\n",
    "        #     \"Popular culture\", \"Music\", \"Television\", \"Film\", \"Video games\"\n",
    "    ]\n",
    "}\n",
    "MAX_DEPTH = 1  # Subcategory traversal depth\n",
    "OUTPUT_CSV = \"category_titles_by_group.csv\""
   ],
   "id": "cdbb963fc4e67753",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:49:56.525648Z",
     "start_time": "2025-07-01T12:49:38.245172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "records = fetch_titles.collect_titles(ROOT_CATEGORIES, MAX_DEPTH)\n",
    "fetch_titles.save_to_csv(records, OUTPUT_CSV)\n",
    "print(f\"Fetched {len(records)} articles across {len(ROOT_CATEGORIES)} groups (depth={MAX_DEPTH}).\")\n"
   ],
   "id": "57441a51dde4a2b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supercategories:   0%|          | 0/1 [00:00<?, ?group/s]\n",
      "Politics:   0%|          | 0/1 [00:00<?, ?cat/s]\u001B[A\n",
      "Politics: 100%|██████████| 1/1 [00:18<00:00, 18.25s/cat]\u001B[A\n",
      "Supercategories: 100%|██████████| 1/1 [00:18<00:00, 18.26s/group]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1643 articles across 1 groups (depth=1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:34:22.100534Z",
     "start_time": "2025-07-01T18:34:22.048832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load from all_articles_by_category.csv if available\n",
    "try:\n",
    "    articles_df = pd.read_csv(\"../all_articles_by_category.csv\")\n",
    "    articles = set(articles_df[\"title\"].tolist())\n",
    "    print(f\"Loaded {len(articles)} articles from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Using previously fetched articles.\")"
   ],
   "id": "dc2f2de6c820432b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27725 articles from CSV.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:53:09.819174Z",
     "start_time": "2025-07-01T12:53:09.803836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the output CSV\n",
    "print(\"reading the output CSV file:\", OUTPUT_CSV)\n",
    "titles_df = pd.read_csv(OUTPUT_CSV)\n",
    "titles_df.head()"
   ],
   "id": "8abd00cae9be1c01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the output CSV file: category_titles_by_group.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  supercategory                    category  \\\n",
       "0      Politics  Animal rights and politics   \n",
       "1      Politics  Animal rights and politics   \n",
       "2      Politics        Clothing in politics   \n",
       "3      Politics        Clothing in politics   \n",
       "4      Politics        Clothing in politics   \n",
       "\n",
       "                                     title    pageid  \n",
       "0              Anarchism and animal rights   4398733  \n",
       "1                          Green anarchism     98514  \n",
       "2                                  Abacost   6011479  \n",
       "3  Act respecting the laicity of the State  60358521  \n",
       "4                                  Armband   1446987  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supercategory</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>pageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Animal rights and politics</td>\n",
       "      <td>Anarchism and animal rights</td>\n",
       "      <td>4398733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Animal rights and politics</td>\n",
       "      <td>Green anarchism</td>\n",
       "      <td>98514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Abacost</td>\n",
       "      <td>6011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Act respecting the laicity of the State</td>\n",
       "      <td>60358521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Armband</td>\n",
       "      <td>1446987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch revisions for articles in the CSV file",
   "id": "851d0fedfec74389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:45:58.995350Z",
     "start_time": "2025-07-01T18:45:58.987243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "ARTICLES_CSV = OUTPUT_CSV  #\"mini_articles_by_category.csv\"\n",
    "OUTPUT_REVS_PICKLE = \"SMOL_revision_snapshots.pkl\"\n",
    "CHECKPOINT_DIR = \"checkpoints/revisions\"\n",
    "START_TS = \"2022-01-01T00:00:00Z\"\n",
    "END_TS = \"2024-01-31T23:59:59Z\"\n",
    "FREQ = \"1ME\"  # monthly snapshots"
   ],
   "id": "a913ed6688d81bb1",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:18:27.063064Z",
     "start_time": "2025-07-01T12:54:14.302919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fetch_revisions\n",
    "\n",
    "# Load mini-articles sample\n",
    "articles_df = pd.read_csv(ARTICLES_CSV)\n",
    "titles = articles_df['title'].unique().tolist()\n",
    "\n",
    "# Process in parallel with progress\n",
    "rev_dfs = fetch_revisions.process_batch_with_progress(\n",
    "    fetch_revisions.fetch_revision_snapshots,\n",
    "    titles,\n",
    "    desc=\"Fetching revision snapshots\",\n",
    "    use_threads=True,\n",
    "    cpu_intensive=False,\n",
    "    max_workers=8,\n",
    "    batch_size=10,\n",
    "    carry_forward=True\n",
    ")\n",
    "\n",
    "# Concatenate all results\n",
    "all_revs = pd.concat(rev_dfs, ignore_index=True)\n",
    "print(f\"Fetched {len(all_revs)} revision snapshots for {len(titles)} articles.\")\n",
    "\n",
    "\n",
    "# add the supercategory and category columns back to all_revs DataFrame on pageid\n",
    "def add_categories_to_revisions(revs_df: pd.DataFrame, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a mapping from pageid to supercategory and category\n",
    "    category_map = articles_df.set_index('pageid')[['supercategory', 'category']].to_dict(orient='index')\n",
    "\n",
    "    # Map the categories to the revisions DataFrame\n",
    "    revs_df['supercategory'] = revs_df['pageid'].map(lambda x: category_map.get(x, {}).get('supercategory', None))\n",
    "    revs_df['category'] = revs_df['pageid'].map(lambda x: category_map.get(x, {}).get('category', None))\n",
    "\n",
    "    return revs_df\n",
    "\n",
    "\n",
    "all_revs = add_categories_to_revisions(all_revs, articles_df)\n",
    "\n",
    "# Save final DataFrame\n",
    "all_revs.to_pickle(OUTPUT_REVS_PICKLE)\n",
    "print(f\"Saved combined snapshots to {OUTPUT_REVS_PICKLE}\")"
   ],
   "id": "cb4164d4832250f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching revision snapshots:   0%|          | 0/1548 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80ffdc2a606b48928e3fec3c5590a39e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/155 (10 items) with 8 workers\n",
      "Processing batch 2/155 (10 items) with 8 workers\n",
      "Processing batch 3/155 (10 items) with 8 workers\n",
      "Processing batch 4/155 (10 items) with 8 workers\n",
      "Processing batch 5/155 (10 items) with 8 workers\n",
      "Processing batch 6/155 (10 items) with 8 workers\n",
      "Processing batch 7/155 (10 items) with 8 workers\n",
      "Processing batch 8/155 (10 items) with 8 workers\n",
      "Processing batch 9/155 (10 items) with 8 workers\n",
      "Processing batch 10/155 (10 items) with 8 workers\n",
      "Processing batch 11/155 (10 items) with 8 workers\n",
      "Processing batch 12/155 (10 items) with 8 workers\n",
      "Processing batch 13/155 (10 items) with 8 workers\n",
      "Processing batch 14/155 (10 items) with 8 workers\n",
      "Processing batch 15/155 (10 items) with 8 workers\n",
      "Processing batch 16/155 (10 items) with 8 workers\n",
      "Processing batch 17/155 (10 items) with 8 workers\n",
      "Processing batch 18/155 (10 items) with 8 workers\n",
      "Processing batch 19/155 (10 items) with 8 workers\n",
      "Processing batch 20/155 (10 items) with 8 workers\n",
      "Processing batch 21/155 (10 items) with 8 workers\n",
      "Processing batch 22/155 (10 items) with 8 workers\n",
      "Processing batch 23/155 (10 items) with 8 workers\n",
      "Processing batch 24/155 (10 items) with 8 workers\n",
      "Processing batch 25/155 (10 items) with 8 workers\n",
      "Processing batch 26/155 (10 items) with 8 workers\n",
      "Processing batch 27/155 (10 items) with 8 workers\n",
      "Processing batch 28/155 (10 items) with 8 workers\n",
      "Processing batch 29/155 (10 items) with 8 workers\n",
      "Processing batch 30/155 (10 items) with 8 workers\n",
      "Processing batch 31/155 (10 items) with 8 workers\n",
      "Processing batch 32/155 (10 items) with 8 workers\n",
      "Processing batch 33/155 (10 items) with 8 workers\n",
      "Processing batch 34/155 (10 items) with 8 workers\n",
      "Processing batch 35/155 (10 items) with 8 workers\n",
      "Processing batch 36/155 (10 items) with 8 workers\n",
      "Processing batch 37/155 (10 items) with 8 workers\n",
      "Processing batch 38/155 (10 items) with 8 workers\n",
      "Processing batch 39/155 (10 items) with 8 workers\n",
      "Processing batch 40/155 (10 items) with 8 workers\n",
      "Processing batch 41/155 (10 items) with 8 workers\n",
      "Processing batch 42/155 (10 items) with 8 workers\n",
      "Processing batch 43/155 (10 items) with 8 workers\n",
      "Processing batch 44/155 (10 items) with 8 workers\n",
      "Processing batch 45/155 (10 items) with 8 workers\n",
      "Processing batch 46/155 (10 items) with 8 workers\n",
      "Processing batch 47/155 (10 items) with 8 workers\n",
      "Processing batch 48/155 (10 items) with 8 workers\n",
      "Processing batch 49/155 (10 items) with 8 workers\n",
      "Processing batch 50/155 (10 items) with 8 workers\n",
      "Processing batch 51/155 (10 items) with 8 workers\n",
      "Processing batch 52/155 (10 items) with 8 workers\n",
      "Processing batch 53/155 (10 items) with 8 workers\n",
      "Processing batch 54/155 (10 items) with 8 workers\n",
      "Processing batch 55/155 (10 items) with 8 workers\n",
      "Processing batch 56/155 (10 items) with 8 workers\n",
      "Processing batch 57/155 (10 items) with 8 workers\n",
      "Processing batch 58/155 (10 items) with 8 workers\n",
      "Processing batch 59/155 (10 items) with 8 workers\n",
      "Processing batch 60/155 (10 items) with 8 workers\n",
      "Processing batch 61/155 (10 items) with 8 workers\n",
      "Processing batch 62/155 (10 items) with 8 workers\n",
      "Processing batch 63/155 (10 items) with 8 workers\n",
      "Processing batch 64/155 (10 items) with 8 workers\n",
      "Processing batch 65/155 (10 items) with 8 workers\n",
      "Processing batch 66/155 (10 items) with 8 workers\n",
      "Processing batch 67/155 (10 items) with 8 workers\n",
      "Retry 1/5 for item 8 in batch 67: [Errno 22] Invalid argument: 'checkpoints/revisions\\\\What Is a Nation?.pkl'\n",
      "Retry 2/5 for item 8 in batch 67: [Errno 22] Invalid argument: 'checkpoints/revisions\\\\What Is a Nation?.pkl'\n",
      "Retry 3/5 for item 8 in batch 67: [Errno 22] Invalid argument: 'checkpoints/revisions\\\\What Is a Nation?.pkl'\n",
      "Retry 4/5 for item 8 in batch 67: [Errno 22] Invalid argument: 'checkpoints/revisions\\\\What Is a Nation?.pkl'\n",
      "Error processing item 8 in batch 67 after 5 retries: [Errno 22] Invalid argument: 'checkpoints/revisions\\\\What Is a Nation?.pkl'\n",
      "Processing batch 68/155 (10 items) with 8 workers\n",
      "Processing batch 69/155 (10 items) with 8 workers\n",
      "Processing batch 70/155 (10 items) with 8 workers\n",
      "Processing batch 71/155 (10 items) with 8 workers\n",
      "Processing batch 72/155 (10 items) with 8 workers\n",
      "Processing batch 73/155 (10 items) with 8 workers\n",
      "Processing batch 74/155 (10 items) with 8 workers\n",
      "Processing batch 75/155 (10 items) with 8 workers\n",
      "Processing batch 76/155 (10 items) with 8 workers\n",
      "Processing batch 77/155 (10 items) with 8 workers\n",
      "Processing batch 78/155 (10 items) with 8 workers\n",
      "Processing batch 79/155 (10 items) with 8 workers\n",
      "Processing batch 80/155 (10 items) with 8 workers\n",
      "Processing batch 81/155 (10 items) with 8 workers\n",
      "Processing batch 82/155 (10 items) with 8 workers\n",
      "Processing batch 83/155 (10 items) with 8 workers\n",
      "Processing batch 84/155 (10 items) with 8 workers\n",
      "Processing batch 85/155 (10 items) with 8 workers\n",
      "Processing batch 86/155 (10 items) with 8 workers\n",
      "Processing batch 87/155 (10 items) with 8 workers\n",
      "Processing batch 88/155 (10 items) with 8 workers\n",
      "Processing batch 89/155 (10 items) with 8 workers\n",
      "Processing batch 90/155 (10 items) with 8 workers\n",
      "Processing batch 91/155 (10 items) with 8 workers\n",
      "Processing batch 92/155 (10 items) with 8 workers\n",
      "Processing batch 93/155 (10 items) with 8 workers\n",
      "Processing batch 94/155 (10 items) with 8 workers\n",
      "Processing batch 95/155 (10 items) with 8 workers\n",
      "Processing batch 96/155 (10 items) with 8 workers\n",
      "Processing batch 97/155 (10 items) with 8 workers\n",
      "Processing batch 98/155 (10 items) with 8 workers\n",
      "Processing batch 99/155 (10 items) with 8 workers\n",
      "Processing batch 100/155 (10 items) with 8 workers\n",
      "Processing batch 101/155 (10 items) with 8 workers\n",
      "Processing batch 102/155 (10 items) with 8 workers\n",
      "Processing batch 103/155 (10 items) with 8 workers\n",
      "Processing batch 104/155 (10 items) with 8 workers\n",
      "Processing batch 105/155 (10 items) with 8 workers\n",
      "Processing batch 106/155 (10 items) with 8 workers\n",
      "Processing batch 107/155 (10 items) with 8 workers\n",
      "Processing batch 108/155 (10 items) with 8 workers\n",
      "Processing batch 109/155 (10 items) with 8 workers\n",
      "Processing batch 110/155 (10 items) with 8 workers\n",
      "Processing batch 111/155 (10 items) with 8 workers\n",
      "Processing batch 112/155 (10 items) with 8 workers\n",
      "Processing batch 113/155 (10 items) with 8 workers\n",
      "Processing batch 114/155 (10 items) with 8 workers\n",
      "Processing batch 115/155 (10 items) with 8 workers\n",
      "Processing batch 116/155 (10 items) with 8 workers\n",
      "Processing batch 117/155 (10 items) with 8 workers\n",
      "Processing batch 118/155 (10 items) with 8 workers\n",
      "Processing batch 119/155 (10 items) with 8 workers\n",
      "Processing batch 120/155 (10 items) with 8 workers\n",
      "Processing batch 121/155 (10 items) with 8 workers\n",
      "Processing batch 122/155 (10 items) with 8 workers\n",
      "Processing batch 123/155 (10 items) with 8 workers\n",
      "Processing batch 124/155 (10 items) with 8 workers\n",
      "Processing batch 125/155 (10 items) with 8 workers\n",
      "Processing batch 126/155 (10 items) with 8 workers\n",
      "Processing batch 127/155 (10 items) with 8 workers\n",
      "Processing batch 128/155 (10 items) with 8 workers\n",
      "Processing batch 129/155 (10 items) with 8 workers\n",
      "Processing batch 130/155 (10 items) with 8 workers\n",
      "Processing batch 131/155 (10 items) with 8 workers\n",
      "Processing batch 132/155 (10 items) with 8 workers\n",
      "Processing batch 133/155 (10 items) with 8 workers\n",
      "Processing batch 134/155 (10 items) with 8 workers\n",
      "Processing batch 135/155 (10 items) with 8 workers\n",
      "Processing batch 136/155 (10 items) with 8 workers\n",
      "Processing batch 137/155 (10 items) with 8 workers\n",
      "Processing batch 138/155 (10 items) with 8 workers\n",
      "Processing batch 139/155 (10 items) with 8 workers\n",
      "Processing batch 140/155 (10 items) with 8 workers\n",
      "Processing batch 141/155 (10 items) with 8 workers\n",
      "Processing batch 142/155 (10 items) with 8 workers\n",
      "Processing batch 143/155 (10 items) with 8 workers\n",
      "Processing batch 144/155 (10 items) with 8 workers\n",
      "Processing batch 145/155 (10 items) with 8 workers\n",
      "Processing batch 146/155 (10 items) with 8 workers\n",
      "Processing batch 147/155 (10 items) with 8 workers\n",
      "Processing batch 148/155 (10 items) with 8 workers\n",
      "Processing batch 149/155 (10 items) with 8 workers\n",
      "Processing batch 150/155 (10 items) with 8 workers\n",
      "Processing batch 151/155 (10 items) with 8 workers\n",
      "Processing batch 152/155 (10 items) with 8 workers\n",
      "Processing batch 153/155 (10 items) with 8 workers\n",
      "Processing batch 154/155 (10 items) with 8 workers\n",
      "Processing batch 155/155 (8 items) with 8 workers\n",
      "Fetched 28169 revision snapshots for 1548 articles.\n",
      "Saved combined snapshots to SMOL_revision_snapshots.pkl\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add the supercategory and category columns back to all_revs DataFrame on pageid\n",
    "def add_categories_to_revisions(revs_df: pd.DataFrame,\n",
    "                                articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins supercategory and category onto revs_df by pageid,\n",
    "    dropping any pre-existing category columns so you only end up\n",
    "    with one clean set.\n",
    "    \"\"\"\n",
    "    # 1. Drop any old columns in revs_df\n",
    "    revs_df = revs_df.drop(columns=[\"supercategory\", \"category\"], errors=\"ignore\")\n",
    "\n",
    "    # 2. Reduce articles_df to one row per pageid\n",
    "    unique_articles = (\n",
    "        articles_df\n",
    "        .drop_duplicates(subset=[\"pageid\"])\n",
    "        .loc[:, [\"pageid\", \"supercategory\", \"category\"]]\n",
    "    )\n",
    "\n",
    "    # 3. Left-merge in the one copy of each category\n",
    "    merged = revs_df.merge(\n",
    "        unique_articles,\n",
    "        on=\"pageid\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "all_revs = add_categories_to_revisions(all_revs, articles_df)\n",
    "\n",
    "# Save final DataFrame\n",
    "all_revs.to_pickle(OUTPUT_REVS_PICKLE)\n",
    "print(f\"Saved combined snapshots to {OUTPUT_REVS_PICKLE}\")\n",
    "\n",
    "# load the all_revs DataFrame if needed\n",
    "revisions = pd.read_pickle(\"SMOL_revision_snapshots.pkl\")\n",
    "revisions.head()\n"
   ],
   "id": "dc9eb6d3eecde58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print average revisions per article\n",
    "avg_revisions = revisions.groupby('pageid').size().mean()\n",
    "stdev_revisions = revisions.groupby('pageid').size().std()\n",
    "print(f\"Average revisions per article: {avg_revisions:.2f}\")\n",
    "print(f\"Standard deviation of revisions per article: {stdev_revisions:.2f}\")\n",
    "\n",
    "print(f\"Total number of revisions: {len(revisions)}\")\n",
    "print(f\"distinct articles: {revisions['pageid'].nunique()}\")\n",
    "print(f\"distinct categories: {revisions['category'].nunique()}\")\n",
    "print(f\"distinct supercategories: {revisions['supercategory'].nunique()}\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime.fromisoformat(START_TS[:-1])  # remove 'Z' for datetime\n",
    "end_date = datetime.fromisoformat(END_TS[:-1])  # remove 'Z' for datetime\n",
    "num_months = (end_date.year - start_date.year) * 12 + end_date.month - start_date.month + 1\n",
    "print(f\"Number of months in interval: {num_months}\")"
   ],
   "id": "4b396e4ff2817093"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:33:32.023003Z",
     "start_time": "2025-07-01T18:33:32.013524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(wikitext: str) -> str:\n",
    "    \"\"\"Clean Wikipedia markup text to plain text\"\"\"\n",
    "    if not isinstance(wikitext, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove wiki markup—keep plain text for prototype\n",
    "    text = re.sub(r\"<ref>.*?</ref>\", \"\", wikitext, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text)  # simple template removal\n",
    "    text = re.sub(r\"\\[\\[([^|\\]]*\\|)?([^\\]]+)\\]\\]\", r\"\\2\", text)  # keep link text\n",
    "    text = re.sub(r\"''+\", \"\", text)  # remove italic/bold\n",
    "    # Remove non-alphabetic chars except basic punctuation\n",
    "    text = re.sub(r\"[^A-Za-z0-9 \\.\\,\\!\\?\\-\\'\\\"]+ \", \" \", text)\n",
    "    # Lowercase and collapse whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ],
   "id": "635bafe232aef828",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Clean text in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    clean_text,\n",
    "    column=\"content\",\n",
    "    new_column=\"plain_text\",\n",
    "    use_threads=True,  # Text cleaning is I/O-bound\n",
    "    cpu_intensive=False\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "178acc7e123ec1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display sample\n",
    "all_revs[[\"title\", \"plain_text\"]].head(11)"
   ],
   "id": "e02830afa1f569e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SpaCy parsing of cleaned text",
   "id": "ae1b48ea04721d09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spaCyParser\n",
    "\n",
    "result_spacy = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    spaCyParser.parse_with_spacy,\n",
    "    column=\"plain_text\",\n",
    "    new_column=\"parsed\",\n",
    "    use_threads=True,\n",
    "    cpu_intensive=False\n",
    "\n",
    ")\n",
    "result_spacy.head()"
   ],
   "id": "bba0dd8a9c1a1d0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save results to a new CSV file\n",
    "result_spacy.to_csv(\"parsed_revisions.csv\", index=False)"
   ],
   "id": "2bb64be4c7371e51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Delta word frequency",
   "id": "f628cdf00b981b95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from lexical_spike import load_trigger_set, compute_baseline_q, add_lexical_spike_delta\n",
    "\n",
    "# 1. Load your data\n",
    "all_revs = pd.read_csv(\"after_spacy_parsed100percat_with_categories_june23.csv\")\n",
    "\n",
    "# 2. Load trigger words\n",
    "trigger_set = load_trigger_set(\"combined_chatgpt_words.csv\")\n",
    "\n",
    "# 3. Compute baseline q\n",
    "q = compute_baseline_q(all_revs, trigger_set, cutoff_date=\"2022-11-01\")\n",
    "print(f\"Baseline q: {q:.6f}\")\n",
    "\n",
    "# 4. Add p_t and delta\n",
    "result_lexical_spike = add_lexical_spike_delta(all_revs, q, trigger_set)\n",
    "\n",
    "# 5. Save or inspect\n",
    "result_lexical_spike.to_csv(\"lexical_spikes.csv\", index=False)\n"
   ],
   "id": "4f241b9d69f43aa6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Perplexity and burstiness",
   "id": "e5d2c2d0d47bf281"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T19:07:33.262158Z",
     "start_time": "2025-07-01T19:07:33.128248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gpt2_perplexity_burstiness import add_perplexity_and_burstiness_to_df\n",
    "\n",
    "# load your revisions DataFrame however you like\n",
    "all_revs = pd.read_csv(\"after_spacy_parsed100percat_with_categories_june23.csv\")\n",
    "\n",
    "# this will add .perplexity and .burstiness columns in place\n",
    "all_revs = add_perplexity_and_burstiness_to_df(\n",
    "    all_revs,\n",
    "    text_col=\"plain_text\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# inspect\n",
    "all_revs.head()\n"
   ],
   "id": "47ec5171db92f437",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPT2LMHeadModel' from 'transformers' (C:\\Users\\david\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[69]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgpt2_perplexity_burstiness\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m add_perplexity_and_burstiness_to_df\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# load your revisions DataFrame however you like\u001B[39;00m\n\u001B[32m      5\u001B[39m all_revs = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33mafter_spacy_parsed100percat_with_categories_june23.csv\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\WikiMScThesis\\python_code\\revised_pipeline\\gpt2_perplexity_burstiness.py:4\u001B[39m\n\u001B[32m      1\u001B[39m from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n\u001B[32m      2\u001B[39m import torch\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m # Check GPU availability\n\u001B[32m      5\u001B[39m print(f\"PyTorch version: {torch.__version__}\")\n\u001B[32m      6\u001B[39m print(f\"CUDA version: {torch.version.cuda}\")\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'GPT2LMHeadModel' from 'transformers' (C:\\Users\\david\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract attributes from parsed dictionaries\n",
    "all_revs[\"upos_props\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"upos_props\", {}))\n",
    "all_revs[\"mean_dep_depth\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"mean_dep_depth\", 0))\n",
    "all_revs[\"clause_ratio\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"clause_ratio\", 0))\n",
    "all_revs[\"voice_ratio\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"voice_ratio\", 0))\n"
   ],
   "id": "3c0d2d1678b35d53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from textstat import textstat\n",
    "\n",
    "\n",
    "def compute_readability(text: str):\n",
    "    \"\"\"Compute readability metrics for text\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        fre = textstat.flesch_reading_ease(text)\n",
    "        fog = textstat.gunning_fog(text)\n",
    "\n",
    "        # Characters per sentence\n",
    "        sentences = list(nlp(text).sents)\n",
    "        chars_per_sent = sum(len(sent.text) for sent in sentences) / (len(sentences) or 1)\n",
    "\n",
    "        # Sentences per paragraph (since we have flattened text, treat the entire text as one paragraph)\n",
    "        sents_per_para = len(sentences)  # toy assumption: 1 paragraph = all sentences\n",
    "\n",
    "        return fre, fog, chars_per_sent, sents_per_para\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing readability: {str(e)}\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n"
   ],
   "id": "91b5692a23f78a44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute readability metrics in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    compute_readability,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"fre\", \"fog\", \"chars_per_sent\", \"sents_per_para\"],\n",
    "    use_threads=True,  # CPU-intensive\n",
    "    cpu_intensive=True\n",
    ")\n"
   ],
   "id": "f2e18abdef32aef3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_vocab_diversity(text: str, window_size: int = 250):\n",
    "    \"\"\"Compute vocabulary diversity metrics\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    tokens = text.split()[:window_size]\n",
    "    unique_count = len(set(tokens))\n",
    "    total_count = len(tokens) or 1\n",
    "\n",
    "    # Normalized TTR = unique / sqrt(2 * total)\n",
    "    nTTR = unique_count / ((2 * total_count) ** 0.5)\n",
    "\n",
    "    # Word-density: lines = count of '\\n' + 1, avg_line_len:\n",
    "    lines = text.count(\"\\n\") + 1\n",
    "    avg_line_len = sum(len(line) for line in text.split(\"\\n\")) / lines\n",
    "    wd = 100 * unique_count / (lines * (avg_line_len or 1))\n",
    "\n",
    "    return nTTR, wd\n"
   ],
   "id": "e9108da1cff98b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute vocabulary diversity in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    compute_vocab_diversity,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"nTTR\", \"word_density\"],\n",
    "    use_threads=True  # This is lightweight\n",
    ")\n"
   ],
   "id": "fca1d590968098eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_citation_delta(wikitext: str):\n",
    "    \"\"\"Compute citation delta\"\"\"\n",
    "    if not isinstance(wikitext, str) or not wikitext.strip():\n",
    "        return 0.0\n",
    "\n",
    "    # Count <ref> tags in raw wikitext\n",
    "    added = len(re.findall(r\"<ref[^>]*>\", wikitext))\n",
    "    removed = 0  # For prototype, assume no diff stored; set removed = 0\n",
    "    tokens_changed = len(wikitext.split()) or 1\n",
    "    return (added - removed) / tokens_changed\n",
    "\n",
    "\n",
    "# Compute citation delta\n",
    "all_revs[\"citation_delta\"] = all_revs[\"content\"].apply(compute_citation_delta)"
   ],
   "id": "766abc3ae4d03bd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save everything to a single file\n",
    "all_revs.to_pickle(\"FINAL.pkl\")\n",
    "all_revs.to_csv(\"FINAL.csv\", index=False)"
   ],
   "id": "68de648f3403f551"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
