{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch article titles from root categories and their subcategories",
   "id": "ec42155fbbc04ee1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:32:13.858036Z",
     "start_time": "2025-07-01T17:32:13.849648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fetch_titles, fetch_revisions\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import parallel\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ],
   "id": "f9542147462dabba",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:49:35.859032Z",
     "start_time": "2025-07-01T12:49:35.853305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "# Map of supercategories to lists of root categories (without 'Category:' prefix)\n",
    "ROOT_CATEGORIES: Dict[str, List[str]] = {\n",
    "    \"Politics\": [\n",
    "        \"Politics\"  , \"Political history\", \"Elections\", \"Political parties\"\n",
    "        ],\n",
    "        \"Science & Medicine\": [\n",
    "            \"Science\", \"Medicine\", \"Biology\", \"Physics\", \"Chemistry\"\n",
    "        ],\n",
    "        \"History\": [\n",
    "            \"History\", \"Military history\", \"History by country\"\n",
    "        ],\n",
    "        \"Technology\": [\n",
    "            \"Technology\", \"Computing\", \"Engineering\"\n",
    "        ],\n",
    "        \"Popular Culture\": [\n",
    "            \"Popular culture\", \"Music\", \"Television\", \"Film\", \"Video games\"\n",
    "    ]\n",
    "}\n",
    "MAX_DEPTH = 1  # Subcategory traversal depth\n",
    "OUTPUT_CSV = \"category_titles_by_group.csv\""
   ],
   "id": "cdbb963fc4e67753",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:49:56.525648Z",
     "start_time": "2025-07-01T12:49:38.245172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "records = fetch_titles.collect_titles(ROOT_CATEGORIES, MAX_DEPTH)\n",
    "fetch_titles.save_to_csv(records, OUTPUT_CSV)\n",
    "print(f\"Fetched {len(records)} articles across {len(ROOT_CATEGORIES)} groups (depth={MAX_DEPTH}).\")\n"
   ],
   "id": "57441a51dde4a2b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Supercategories:   0%|          | 0/1 [00:00<?, ?group/s]\n",
      "Politics:   0%|          | 0/1 [00:00<?, ?cat/s]\u001B[A\n",
      "Politics: 100%|██████████| 1/1 [00:18<00:00, 18.25s/cat]\u001B[A\n",
      "Supercategories: 100%|██████████| 1/1 [00:18<00:00, 18.26s/group]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1643 articles across 1 groups (depth=1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T13:10:45.697161Z",
     "start_time": "2025-07-05T13:10:45.678012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chceck the output CSV\n",
    "import pandas as pd\n",
    "print(\"reading the output CSV file:\", OUTPUT_CSV)\n",
    "titles_df = pd.read_csv(OUTPUT_CSV)\n",
    "print(len(titles_df))  # number of articles\n",
    "titles_df.head()\n"
   ],
   "id": "1f4746ba3049cd3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the output CSV file: category_titles_by_group.csv\n",
      "1643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  supercategory                    category  \\\n",
       "0      Politics  Animal rights and politics   \n",
       "1      Politics  Animal rights and politics   \n",
       "2      Politics        Clothing in politics   \n",
       "3      Politics        Clothing in politics   \n",
       "4      Politics        Clothing in politics   \n",
       "\n",
       "                                     title    pageid  \n",
       "0              Anarchism and animal rights   4398733  \n",
       "1                          Green anarchism     98514  \n",
       "2                                  Abacost   6011479  \n",
       "3  Act respecting the laicity of the State  60358521  \n",
       "4                                  Armband   1446987  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supercategory</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>pageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Animal rights and politics</td>\n",
       "      <td>Anarchism and animal rights</td>\n",
       "      <td>4398733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Animal rights and politics</td>\n",
       "      <td>Green anarchism</td>\n",
       "      <td>98514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Abacost</td>\n",
       "      <td>6011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Act respecting the laicity of the State</td>\n",
       "      <td>60358521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politics</td>\n",
       "      <td>Clothing in politics</td>\n",
       "      <td>Armband</td>\n",
       "      <td>1446987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:34:22.100534Z",
     "start_time": "2025-07-01T18:34:22.048832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load from all_articles_by_category.csv if available\n",
    "try:\n",
    "    articles_df = pd.read_csv(\"../all_articles_by_category.csv\")\n",
    "    articles = set(articles_df[\"title\"].tolist())\n",
    "    print(f\"Loaded {len(articles)} articles from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Using previously fetched articles.\")"
   ],
   "id": "dc2f2de6c820432b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27725 articles from CSV.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check the output CSV\n",
    "print(\"reading the output CSV file:\", OUTPUT_CSV)\n",
    "titles_df = pd.read_csv(OUTPUT_CSV)\n",
    "titles_df.head()\n",
    "\n"
   ],
   "id": "6e323dd4ba65444b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch revisions for articles in the CSV file",
   "id": "df1ff0ac6213fe54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "ARTICLES_CSV = OUTPUT_CSV  #\"mini_articles_by_category.csv\"\n",
    "OUTPUT_REVS_PICKLE = \"SMOL_revision_snapshots.pkl\"\n",
    "CHECKPOINT_DIR = \"checkpoints/revisions\"\n",
    "START_TS = \"2022-01-01T00:00:00Z\"\n",
    "END_TS = \"2024-01-31T23:59:59Z\"\n",
    "FREQ = \"1ME\"  # monthly snapshots"
   ],
   "id": "425fd06ab510aed7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import fetch_revisions\n",
    "\n",
    "# Load mini-articles sample\n",
    "articles_df = pd.read_csv(ARTICLES_CSV)\n",
    "titles = articles_df['title'].unique().tolist()\n",
    "\n",
    "# Process in parallel with progress\n",
    "rev_dfs = fetch_revisions.process_batch_with_progress(\n",
    "    fetch_revisions.fetch_revision_snapshots,\n",
    "    titles,\n",
    "    desc=\"Fetching revision snapshots\",\n",
    "    use_threads=True,\n",
    "    cpu_intensive=False,\n",
    "    max_workers=8,\n",
    "    batch_size=10,\n",
    "    carry_forward=True\n",
    ")\n",
    "\n",
    "# Concatenate all results\n",
    "all_revs = pd.concat(rev_dfs, ignore_index=True)\n",
    "print(f\"Fetched {len(all_revs)} revision snapshots for {len(titles)} articles.\")\n",
    "\n",
    "\n",
    "# add the supercategory and category columns back to all_revs DataFrame on pageid\n",
    "def add_categories_to_revisions(revs_df: pd.DataFrame, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a mapping from pageid to supercategory and category\n",
    "    category_map = articles_df.set_index('pageid')[['supercategory', 'category']].to_dict(orient='index')\n",
    "\n",
    "    # Map the categories to the revisions DataFrame\n",
    "    revs_df['supercategory'] = revs_df['pageid'].map(lambda x: category_map.get(x, {}).get('supercategory', None))\n",
    "    revs_df['category'] = revs_df['pageid'].map(lambda x: category_map.get(x, {}).get('category', None))\n",
    "\n",
    "    return revs_df\n",
    "\n",
    "\n",
    "all_revs = add_categories_to_revisions(all_revs, articles_df)\n",
    "\n",
    "# Save final DataFrame\n",
    "all_revs.to_pickle(OUTPUT_REVS_PICKLE)\n",
    "print(f\"Saved combined snapshots to {OUTPUT_REVS_PICKLE}\")"
   ],
   "id": "a77018b830670e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# add the supercategory and category columns back to all_revs DataFrame on pageid\n",
    "def add_categories_to_revisions(revs_df: pd.DataFrame,\n",
    "                                articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Joins supercategory and category onto revs_df by pageid,\n",
    "    dropping any pre-existing category columns so you only end up\n",
    "    with one clean set.\n",
    "    \"\"\"\n",
    "    # 1. Drop any old columns in revs_df\n",
    "    revs_df = revs_df.drop(columns=[\"supercategory\", \"category\"], errors=\"ignore\")\n",
    "\n",
    "    # 2. Reduce articles_df to one row per pageid\n",
    "    unique_articles = (\n",
    "        articles_df\n",
    "        .drop_duplicates(subset=[\"pageid\"])\n",
    "        .loc[:, [\"pageid\", \"supercategory\", \"category\"]]\n",
    "    )\n",
    "\n",
    "    # 3. Left-merge in the one copy of each category\n",
    "    merged = revs_df.merge(\n",
    "        unique_articles,\n",
    "        on=\"pageid\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "all_revs = add_categories_to_revisions(all_revs, articles_df)\n",
    "\n",
    "# Save final DataFrame\n",
    "all_revs.to_pickle(OUTPUT_REVS_PICKLE)\n",
    "print(f\"Saved combined snapshots to {OUTPUT_REVS_PICKLE}\")\n",
    "\n",
    "# load the all_revs DataFrame if needed\n",
    "revisions = pd.read_pickle(\"SMOL_revision_snapshots.pkl\")\n",
    "revisions.head()\n"
   ],
   "id": "be01fba311bb9a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print average revisions per article\n",
    "avg_revisions = revisions.groupby('pageid').size().mean()\n",
    "stdev_revisions = revisions.groupby('pageid').size().std()\n",
    "print(f\"Average revisions per article: {avg_revisions:.2f}\")\n",
    "print(f\"Standard deviation of revisions per article: {stdev_revisions:.2f}\")\n",
    "\n",
    "print(f\"Total number of revisions: {len(revisions)}\")\n",
    "print(f\"distinct articles: {revisions['pageid'].nunique()}\")\n",
    "print(f\"distinct categories: {revisions['category'].nunique()}\")\n",
    "print(f\"distinct supercategories: {revisions['supercategory'].nunique()}\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_date = datetime.fromisoformat(START_TS[:-1])  # remove 'Z' for datetime\n",
    "end_date = datetime.fromisoformat(END_TS[:-1])  # remove 'Z' for datetime\n",
    "num_months = (end_date.year - start_date.year) * 12 + end_date.month - start_date.month + 1\n",
    "print(f\"Number of months in interval: {num_months}\")"
   ],
   "id": "111f95735357d1e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(wikitext: str) -> str:\n",
    "    \"\"\"Clean Wikipedia markup text to plain text\"\"\"\n",
    "    if not isinstance(wikitext, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove wiki markup—keep plain text for prototype\n",
    "    text = re.sub(r\"<ref>.*?</ref>\", \"\", wikitext, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text)  # simple template removal\n",
    "    text = re.sub(r\"\\[\\[([^|\\]]*\\|)?([^\\]]+)\\]\\]\", r\"\\2\", text)  # keep link text\n",
    "    text = re.sub(r\"''+\", \"\", text)  # remove italic/bold\n",
    "    # Remove non-alphabetic chars except basic punctuation\n",
    "    text = re.sub(r\"[^A-Za-z0-9 \\.\\,\\!\\?\\-\\'\\\"]+ \", \" \", text)\n",
    "    # Lowercase and collapse whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ],
   "id": "41bbfd8bf3b1dd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Clean text in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    clean_text,\n",
    "    column=\"content\",\n",
    "    new_column=\"plain_text\",\n",
    "    use_threads=True,  # Text cleaning is I/O-bound\n",
    "    cpu_intensive=False\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "2a616cfa6e549152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display sample\n",
    "all_revs[[\"title\", \"plain_text\"]].head(11)"
   ],
   "id": "37fccfb403753f29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SpaCy parsing of cleaned text",
   "id": "5ce8a6bd186af792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spaCyParser\n",
    "\n",
    "result_spacy = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    spaCyParser.parse_with_spacy,\n",
    "    column=\"plain_text\",\n",
    "    new_column=\"parsed\",\n",
    "    use_threads=True,\n",
    "    cpu_intensive=False\n",
    "\n",
    ")\n",
    "result_spacy.head()"
   ],
   "id": "66ca940aa0266371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save results to a new CSV file\n",
    "result_spacy.to_csv(\"parsed_revisions.csv\", index=False)"
   ],
   "id": "75c2d409f3ba71f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Delta word frequency",
   "id": "e2123eb2a0d6e75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from lexical_spike import load_trigger_set, compute_baseline_q, add_lexical_spike_delta\n",
    "\n",
    "# 1. Load your data\n",
    "all_revs = pd.read_csv(\"after_spacy_parsed100percat_with_categories_june23.csv\")\n",
    "\n",
    "# 2. Load trigger words\n",
    "trigger_set = load_trigger_set(\"combined_chatgpt_words.csv\")\n",
    "\n",
    "# 3. Compute baseline q\n",
    "q = compute_baseline_q(all_revs, trigger_set, cutoff_date=\"2022-11-01\")\n",
    "print(f\"Baseline q: {q:.6f}\")\n",
    "\n",
    "# 4. Add p_t and delta\n",
    "result_lexical_spike = add_lexical_spike_delta(all_revs, q, trigger_set)\n",
    "\n",
    "# 5. Save or inspect\n",
    "result_lexical_spike.to_csv(\"lexical_spikes.csv\", index=False)\n"
   ],
   "id": "4ae8cf5b77a3afee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Perplexity and burstiness",
   "id": "709078a656ccaabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from gpt2_perplexity_burstiness import add_perplexity_and_burstiness_to_df\n",
    "\n",
    "# load your revisions DataFrame however you like\n",
    "all_revs = pd.read_csv(\"after_spacy_parsed100percat_with_categories_june23.csv\")\n",
    "\n",
    "# this will add .perplexity and .burstiness columns in place\n",
    "all_revs = add_perplexity_and_burstiness_to_df(\n",
    "    all_revs,\n",
    "    text_col=\"plain_text\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# inspect\n",
    "all_revs.head()\n"
   ],
   "id": "87356e02ac152e0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract attributes from parsed dictionaries\n",
    "all_revs[\"upos_props\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"upos_props\", {}))\n",
    "all_revs[\"mean_dep_depth\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"mean_dep_depth\", 0))\n",
    "all_revs[\"clause_ratio\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"clause_ratio\", 0))\n",
    "all_revs[\"voice_ratio\"] = result_spacy[\"parsed\"].apply(lambda x: x.get(\"voice_ratio\", 0))\n"
   ],
   "id": "e0dd47f78b76b0ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from textstat import textstat\n",
    "\n",
    "\n",
    "def compute_readability(text: str):\n",
    "    \"\"\"Compute readability metrics for text\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        fre = textstat.flesch_reading_ease(text)\n",
    "        fog = textstat.gunning_fog(text)\n",
    "\n",
    "        # Characters per sentence\n",
    "        sentences = list(nlp(text).sents)\n",
    "        chars_per_sent = sum(len(sent.text) for sent in sentences) / (len(sentences) or 1)\n",
    "\n",
    "        # Sentences per paragraph (since we have flattened text, treat the entire text as one paragraph)\n",
    "        sents_per_para = len(sentences)  # toy assumption: 1 paragraph = all sentences\n",
    "\n",
    "        return fre, fog, chars_per_sent, sents_per_para\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing readability: {str(e)}\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n"
   ],
   "id": "96f9338d81b6e102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute readability metrics in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    compute_readability,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"fre\", \"fog\", \"chars_per_sent\", \"sents_per_para\"],\n",
    "    use_threads=True,  # CPU-intensive\n",
    "    cpu_intensive=True\n",
    ")\n"
   ],
   "id": "b63f57474b5b41ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_vocab_diversity(text: str, window_size: int = 250):\n",
    "    \"\"\"Compute vocabulary diversity metrics\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    tokens = text.split()[:window_size]\n",
    "    unique_count = len(set(tokens))\n",
    "    total_count = len(tokens) or 1\n",
    "\n",
    "    # Normalized TTR = unique / sqrt(2 * total)\n",
    "    nTTR = unique_count / ((2 * total_count) ** 0.5)\n",
    "\n",
    "    # Word-density: lines = count of '\\n' + 1, avg_line_len:\n",
    "    lines = text.count(\"\\n\") + 1\n",
    "    avg_line_len = sum(len(line) for line in text.split(\"\\n\")) / lines\n",
    "    wd = 100 * unique_count / (lines * (avg_line_len or 1))\n",
    "\n",
    "    return nTTR, wd\n"
   ],
   "id": "72f77ea3e3198dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute vocabulary diversity in parallel\n",
    "all_revs = parallel.process_dataframe_parallel(\n",
    "    all_revs,\n",
    "    compute_vocab_diversity,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"nTTR\", \"word_density\"],\n",
    "    use_threads=True  # This is lightweight\n",
    ")\n"
   ],
   "id": "2a7c4cebd9ad23c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_citation_delta(wikitext: str):\n",
    "    \"\"\"Compute citation delta\"\"\"\n",
    "    if not isinstance(wikitext, str) or not wikitext.strip():\n",
    "        return 0.0\n",
    "\n",
    "    # Count <ref> tags in raw wikitext\n",
    "    added = len(re.findall(r\"<ref[^>]*>\", wikitext))\n",
    "    removed = 0  # For prototype, assume no diff stored; set removed = 0\n",
    "    tokens_changed = len(wikitext.split()) or 1\n",
    "    return (added - removed) / tokens_changed\n",
    "\n",
    "\n",
    "# Compute citation delta\n",
    "all_revs[\"citation_delta\"] = all_revs[\"content\"].apply(compute_citation_delta)"
   ],
   "id": "14bc61afcac26d83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save everything to a single file\n",
    "all_revs.to_pickle(\"FINAL.pkl\")\n",
    "all_revs.to_csv(\"FINAL.csv\", index=False)"
   ],
   "id": "cedf4963eda70091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "random_rev = pd.read_pickle(\"checkpoints/revisions/Referendum.pkl\")\n",
    "len(random_rev)"
   ],
   "id": "a398348d9a775605"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze the revisions COME BACK LATER",
   "id": "b1fba43ccf546b55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Checks",
   "id": "3152bea57708976e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count the number of articles in the checkpoints/revisions directory\n",
    "import os\n",
    "def count_articles_in_checkpoints(checkpoint_dir: str) -> int:\n",
    "    \"\"\"Count the number of articles in the checkpoints/revisions directory.\"\"\"\n",
    "    return len([f for f in os.listdir(checkpoint_dir) if f.endswith('.pkl')])\n",
    "\n",
    "checkpoint_dir = \"checkpoints/revisions\"\n",
    "num_articles = count_articles_in_checkpoints(checkpoint_dir)\n",
    "print(f\"Number of articles in {checkpoint_dir}: {num_articles}\")\n",
    "\n",
    "# build dataframe of articles in checkpoints\n",
    "def build_articles_df(checkpoint_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Build a DataFrame of articles in the checkpoints/revisions directory.\"\"\"\n",
    "    articles = []\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith('.pkl'):\n",
    "            pageid = filename[:-4]  # remove .pkl\n",
    "            articles.append({\"pageid\": pageid, \"filename\": filename})\n",
    "    return pd.DataFrame(articles)\n",
    "articles_df = build_articles_df(checkpoint_dir)\n",
    "articles_df.to_csv(\"articles_in_checkpoints.csv\", index=False)\n"
   ],
   "id": "5cf1575411e220d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# size of titles_df\n",
    "print(f\"Number of articles in titles_df: {len(titles_df)}\")\n",
    "\n",
    "# join articles_df with titles_df on pageid from articles_df and title from titles_df\n",
    "def join_articles_with_titles(articles_df: pd.DataFrame, titles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join articles DataFrame with titles DataFrame on pageid and title.\"\"\"\n",
    "    articles_df['pageid'] = articles_df['pageid'].astype(str)\n",
    "    titles_df['title'] = titles_df['title'].astype(str)\n",
    "    return articles_df.merge(titles_df, left_on='pageid', right_on='title', how='left')\n",
    "\n",
    "joined_df = join_articles_with_titles(articles_df, titles_df)\n"
   ],
   "id": "23a86e79f139c880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "joined_df",
   "id": "8f419ee9dd3d4a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count articles with category and supercategory different from None/NaN\n",
    "def count_articles_with_categories(df: pd.DataFrame) -> int:\n",
    "    \"\"\"Count articles with non-null supercategory and category.\"\"\"\n",
    "    return df[(df['supercategory'].notna()) & (df['category'].notna())].shape[0]\n",
    "count = count_articles_with_categories(joined_df)\n",
    "print(f\"Number of articles with non-null supercategory and category: {count}\")"
   ],
   "id": "eb4b1d5588b4b84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7bab571d6cf8ce19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
