{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:20:15.329996Z",
     "start_time": "2025-08-30T12:20:15.325523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle"
   ],
   "id": "56fb44abe2453440",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-30T12:20:28.155487Z",
     "start_time": "2025-08-30T12:20:15.992074Z"
    }
   },
   "source": "full_data = pd.read_csv(\"everything100percat.csv\")\n",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:13:20.687994Z",
     "start_time": "2025-08-28T14:13:20.680940Z"
    }
   },
   "cell_type": "code",
   "source": "print(full_data.dtypes)",
   "id": "3eadfa7bee17e5b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0               int64\n",
      "snapshot_ts             object\n",
      "rev_id                   int64\n",
      "timestamp               object\n",
      "user                    object\n",
      "is_bot                    bool\n",
      "content                 object\n",
      "article_id               int64\n",
      "title                   object\n",
      "root                    object\n",
      "stratum                 object\n",
      "plain_text              object\n",
      "p_t                    float64\n",
      "lexical_spike_delta    float64\n",
      "perplexity             float64\n",
      "burstiness             float64\n",
      "upos_props              object\n",
      "mean_dep_depth         float64\n",
      "clause_ratio           float64\n",
      "voice_ratio            float64\n",
      "fre                    float64\n",
      "fog                    float64\n",
      "chars_per_sent         float64\n",
      "sents_per_para         float64\n",
      "nTTR                   float64\n",
      "word_density           float64\n",
      "avg_line_len           float64\n",
      "citation_delta         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:21:23.437711Z",
     "start_time": "2025-08-30T12:21:23.424421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell: Normalization utilities (place near your other utils) ===\n",
    "\n",
    "\n",
    "def compute_baseline_stats(\n",
    "        df: pd.DataFrame,\n",
    "        feature_cols,\n",
    "        category_col: str = 'root',\n",
    "        timestamp_col: str = 'timestamp',\n",
    "        baseline_end_date: str = '2022-11-01',\n",
    "        min_baseline_rows_per_category: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build per-category baseline means/std from rows strictly before baseline_end_date.\n",
    "    Filters out categories with too few baseline rows to avoid junk std=0 artefacts.\n",
    "    \"\"\"\n",
    "    # Ensure datetime\n",
    "    if not np.issubdtype(df[timestamp_col].dtype, np.datetime64):\n",
    "        df = df.copy()\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], utc=True, errors='coerce')\n",
    "\n",
    "    baseline_df = df[df[timestamp_col] < pd.Timestamp(baseline_end_date, tz='UTC')].copy()\n",
    "\n",
    "    # (Optional) guard: drop categories with too few baseline observations\n",
    "    valid_cats = (\n",
    "        baseline_df.groupby(category_col)[feature_cols[0]]\n",
    "        .size()\n",
    "        .loc[lambda s: s >= min_baseline_rows_per_category]\n",
    "        .index\n",
    "    )\n",
    "    baseline_df = baseline_df[baseline_df[category_col].isin(valid_cats)]\n",
    "\n",
    "    # Compute stats\n",
    "    stats = baseline_df.groupby(category_col)[feature_cols].agg(['mean', 'std'])\n",
    "    stats.columns = ['_'.join(col) for col in stats.columns]  # flatten\n",
    "    return stats\n",
    "\n",
    "\n",
    "def normalize_features_by_baseline(\n",
    "        df: pd.DataFrame,\n",
    "        feature_cols,\n",
    "        baseline_stats: pd.DataFrame,\n",
    "        category_col: str = 'root'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge precomputed baseline stats and add <feature>_zscore columns.\n",
    "    Categories without baseline end up with 0 z-scores to avoid NaNs in voting.\n",
    "    \"\"\"\n",
    "    out = df.merge(baseline_stats, on=category_col, how='left')\n",
    "\n",
    "    for col in feature_cols:\n",
    "        mean_col = f'{col}_mean'\n",
    "        std_col = f'{col}_std'\n",
    "        z_col = f'{col}_zscore'\n",
    "\n",
    "        mean_vals = out[mean_col]\n",
    "        std_vals = out[std_col]\n",
    "\n",
    "        # (value - mean)/std, safe divide; if no stats or std=0 → 0.0\n",
    "        out[z_col] = np.divide(\n",
    "            out[col] - mean_vals,\n",
    "            std_vals,\n",
    "            out=np.zeros(len(out), dtype=float),\n",
    "            where=(std_vals.notna() & (std_vals != 0))\n",
    "        )\n",
    "\n",
    "    # drop the merged mean/std columns\n",
    "    out.drop(columns=[c for c in out.columns if any(c.endswith(sfx) for sfx in ('_mean', '_std'))], inplace=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_or_build_baseline(\n",
    "        df: pd.DataFrame,\n",
    "        feature_cols,\n",
    "        category_col: str = 'root',\n",
    "        timestamp_col: str = 'timestamp',\n",
    "        baseline_end_date: str = '2022-11-01',\n",
    "        cache_path: str | Path = './_baseline_stats.pkl',\n",
    "        min_baseline_rows_per_category: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try to load baseline stats from disk; otherwise compute and cache.\n",
    "    \"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    if cache_path.exists():\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            stats = pickle.load(f)\n",
    "    else:\n",
    "        stats = compute_baseline_stats(\n",
    "            df=df,\n",
    "            feature_cols=feature_cols,\n",
    "            category_col=category_col,\n",
    "            timestamp_col=timestamp_col,\n",
    "            baseline_end_date=baseline_end_date,\n",
    "            min_baseline_rows_per_category=min_baseline_rows_per_category,\n",
    "        )\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "    return stats\n"
   ],
   "id": "2cb95d38c44a0296",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:21:28.195757Z",
     "start_time": "2025-08-30T12:21:28.138644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell: Normalize current feature set ===\n",
    "\n",
    "# Which features to normalize (extend to your full list of 14)\n",
    "FEATURES_TO_NORMALIZE = [\n",
    "    \"p_t\",\n",
    "    \"lexical_spike_delta\",\n",
    "    \"perplexity\",\n",
    "    \"burstiness\",\n",
    "    \"mean_dep_depth\",\n",
    "    \"clause_ratio\",\n",
    "    \"voice_ratio\",\n",
    "    \"fre\",\n",
    "    \"fog\",\n",
    "    \"chars_per_sent\",\n",
    "    \"sents_per_para\",\n",
    "    \"avg_line_len\",\n",
    "    \"nTTR\",\n",
    "    \"word_density\",\n",
    "    \"citation_delta\"\n",
    "]\n",
    "\n",
    "BASELINE_END = '2022-11-01'  # end of pre-ChatGPT window\n",
    "\n",
    "# 1) Build/load per-category baseline stats once\n",
    "baseline_stats = load_or_build_baseline(\n",
    "    df=full_data,\n",
    "    feature_cols=FEATURES_TO_NORMALIZE,\n",
    "    category_col='root',\n",
    "    timestamp_col='timestamp',\n",
    "    baseline_end_date=BASELINE_END,\n",
    "    cache_path='./_baseline_stats.pkl',\n",
    "    min_baseline_rows_per_category=10,\n",
    ")\n",
    "\n",
    "# 2) Produce z-scores for the whole panel (pre and post)\n",
    "features_df_z = normalize_features_by_baseline(\n",
    "    df=full_data,\n",
    "    feature_cols=FEATURES_TO_NORMALIZE,\n",
    "    baseline_stats=baseline_stats,\n",
    "    category_col='root',\n",
    ")\n",
    "\n",
    "# 3) (Optional) keep only z-score views for voting\n",
    "Z_FEATURES = [f'{c}_zscore' for c in FEATURES_TO_NORMALIZE]\n",
    "features_for_voting = features_df_z[['timestamp', 'root', 'title', 'rev_id', 'ai_flag'] + Z_FEATURES].copy() \\\n",
    "    if all(c in features_df_z.columns for c in ['title', 'rev_id', 'ai_flag']) \\\n",
    "    else features_df_z[['timestamp', 'root'] + Z_FEATURES].copy()\n"
   ],
   "id": "6ad4a9821d908c14",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:21:34.130997Z",
     "start_time": "2025-08-30T12:21:34.122070Z"
    }
   },
   "cell_type": "code",
   "source": "features_df_z.columns",
   "id": "387bcb778894ea84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'snapshot_ts', 'rev_id', 'timestamp', 'user', 'is_bot',\n",
       "       'content', 'article_id', 'title', 'root', 'stratum', 'plain_text',\n",
       "       'p_t', 'lexical_spike_delta', 'perplexity', 'burstiness', 'upos_props',\n",
       "       'mean_dep_depth', 'clause_ratio', 'voice_ratio', 'fre', 'fog',\n",
       "       'chars_per_sent', 'sents_per_para', 'nTTR', 'word_density',\n",
       "       'avg_line_len', 'citation_delta', 'p_t_zscore',\n",
       "       'lexical_spike_delta_zscore', 'perplexity_zscore', 'burstiness_zscore',\n",
       "       'mean_dep_depth_zscore', 'clause_ratio_zscore', 'voice_ratio_zscore',\n",
       "       'fre_zscore', 'fog_zscore', 'chars_per_sent_zscore',\n",
       "       'sents_per_para_zscore', 'avg_line_len_zscore', 'nTTR_zscore',\n",
       "       'word_density_zscore', 'citation_delta_zscore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:22:32.066283Z",
     "start_time": "2025-08-30T12:22:31.905434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'features_df_z' is the DataFrame that has been processed by your\n",
    "# normalize_features_by_baseline() function.\n",
    "\n",
    "# --- 1. Define the CORRECT list of z-score columns to be used in the voting system ---\n",
    "# This list now matches the 15 features present in your DataFrame.\n",
    "zscore_cols = [\n",
    "    'p_t_zscore',\n",
    "    'lexical_spike_delta_zscore',\n",
    "    'perplexity_zscore',\n",
    "    'burstiness_zscore',\n",
    "    'mean_dep_depth_zscore',\n",
    "    'clause_ratio_zscore',\n",
    "    'voice_ratio_zscore',\n",
    "    'fre_zscore',\n",
    "    'fog_zscore',\n",
    "    'chars_per_sent_zscore',\n",
    "    'sents_per_para_zscore',\n",
    "    'nTTR_zscore',\n",
    "    'word_density_zscore',\n",
    "    'avg_line_len_zscore',\n",
    "    'citation_delta_zscore'\n",
    "]\n",
    "\n",
    "# --- 2. Define the new voting system function based on a Z-Score Cutoff ---\n",
    "def apply_voting_system_zscore(df, z_score_cutoff=2.5, min_votes=3):\n",
    "    \"\"\"\n",
    "    Applies a voting system to flag revisions based on a z-score cutoff.\n",
    "\n",
    "    A \"vote\" is cast for a revision if the absolute value of a feature's\n",
    "    z-score exceeds the specified cutoff.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the z-score columns.\n",
    "        z_score_cutoff (float): The z-score value to use as a threshold.\n",
    "        min_votes (int): The minimum number of votes required to flag a\n",
    "                         revision as potentially AI-generated.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with two new columns:\n",
    "                      'ai_vote_count' and 'ai_flag'.\n",
    "    \"\"\"\n",
    "    print(f\"Applying voting system with Z-Score cutoff: {z_score_cutoff} and minimum votes: {min_votes}\")\n",
    "\n",
    "    # Calculate the number of votes for each revision.\n",
    "    # A vote is cast if the absolute z-score is > cutoff.\n",
    "    # We use abs() because a very low score (e.g., perplexity) can be as\n",
    "    # anomalous as a very high score (e.g., nTTR).\n",
    "    df['ai_vote_count'] = df[zscore_cols].abs().gt(z_score_cutoff).sum(axis=1)\n",
    "\n",
    "    # Flag the revision if the vote count meets the minimum threshold\n",
    "    df['ai_flag'] = df['ai_vote_count'] >= min_votes\n",
    "\n",
    "    print(\"Voting system application complete.\")\n",
    "    return df\n",
    "\n",
    "# --- 3. Apply the new voting system ---\n",
    "# You can easily experiment with these two parameters now.\n",
    "Z_SCORE_THRESHOLD = 3.5\n",
    "MINIMUM_VOTES = 2 # e.g., flag if at least 4 out of 15 features are anomalous\n",
    "\n",
    "final_df = apply_voting_system_zscore(\n",
    "    features_df_z,\n",
    "    z_score_cutoff=Z_SCORE_THRESHOLD,\n",
    "    min_votes=MINIMUM_VOTES\n",
    ")\n",
    "\n",
    "# --- 4. Display the results ---\n",
    "print(\"\\n--- Voting System Results ---\")\n",
    "print(final_df[['rev_id', 'timestamp', 'root', 'ai_vote_count', 'ai_flag']].head())\n",
    "\n",
    "print(\"\\n--- Distribution of Votes ---\")\n",
    "print(final_df['ai_vote_count'].value_counts().sort_index())\n",
    "\n",
    "# Calculate and display the total number of flagged revisions\n",
    "flagged_count = final_df['ai_flag'].sum()\n",
    "total_count = len(final_df)\n",
    "print(f\"\\nTotal revisions flagged as potentially AI-generated: {flagged_count} out of {total_count} ({flagged_count/total_count:.2%})\")\n"
   ],
   "id": "37d4de7439e87527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying voting system with Z-Score cutoff: 3.5 and minimum votes: 2\n",
      "Voting system application complete.\n",
      "\n",
      "--- Voting System Results ---\n",
      "       rev_id                 timestamp     root  ai_vote_count  ai_flag\n",
      "0  1160763005 2023-06-18 16:30:41+00:00  History              3     True\n",
      "1  1166013433 2023-07-18 21:17:52+00:00  History              3     True\n",
      "2  1171485250 2023-08-21 11:13:21+00:00  History              3     True\n",
      "3  1177319025 2023-09-27 04:45:15+00:00  History              3     True\n",
      "4  1181899435 2023-10-25 22:06:39+00:00  History              3     True\n",
      "\n",
      "--- Distribution of Votes ---\n",
      "ai_vote_count\n",
      "0    14352\n",
      "1      867\n",
      "2      399\n",
      "3      147\n",
      "4       40\n",
      "5        5\n",
      "6       22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total revisions flagged as potentially AI-generated: 613 out of 15832 (3.87%)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_df",
   "id": "93d7ce0ce5cd753a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save to a file called normalized_everything100percat_with_ai_votes.csv\n",
    "final_df.to_csv(\"normalized_everything100percat_with_ai_votes.csv\", index=False)"
   ],
   "id": "c82fe9ea0b04704e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T12:21:01.687208Z",
     "start_time": "2025-08-30T12:20:49.866870Z"
    }
   },
   "cell_type": "code",
   "source": "final_df = pd.read_csv(\"normalized_everything100percat_with_ai_votes.csv\")",
   "id": "fb26df8dafbf75f7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T13:37:00.896014Z",
     "start_time": "2025-08-30T13:37:00.807829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'features_df_z' is the DataFrame that has been processed by your\n",
    "# normalize_features_by_baseline() function and contains all z-score columns.\n",
    "\n",
    "# --- 1. Isolate the Baseline Data for Calibration ---\n",
    "# We only want to test our parameters on the data we know should be \"human\".\n",
    "baseline_end_date = '2022-11-01'\n",
    "# Ensure timestamp column is in datetime format for comparison\n",
    "features_df_z['timestamp'] = pd.to_datetime(features_df_z['timestamp'])\n",
    "baseline_df = features_df_z[features_df_z['timestamp'] < baseline_end_date].copy()\n",
    "\n",
    "print(f\"Calibrating thresholds using {len(baseline_df)} baseline revisions (pre-{baseline_end_date}).\")\n",
    "\n",
    "# --- 2. Define the Grid of Parameters to Test ---\n",
    "z_score_cutoffs = [2.0, 2.5, 3.0, 3.5]\n",
    "min_votes_options = [2, 3, 4, 5, 6]\n",
    "\n",
    "# The list of z-score columns to use for voting\n",
    "zscore_cols = [col for col in features_df_z.columns if col.endswith('_zscore')]\n",
    "\n",
    "# --- 3. Perform the Grid Search ---\n",
    "results = []\n",
    "\n",
    "for z_cutoff in z_score_cutoffs:\n",
    "    row = {'z_score_cutoff': z_cutoff}\n",
    "    for min_v in min_votes_options:\n",
    "        # Calculate votes for the current parameter combination\n",
    "        vote_count = baseline_df[zscore_cols].abs().gt(z_cutoff).sum(axis=1)\n",
    "\n",
    "        # Determine the number of revisions flagged\n",
    "        flagged_count = (vote_count >= min_v).sum()\n",
    "\n",
    "        # Calculate the percentage of the baseline that was flagged (Baseline Flag Rate)\n",
    "        baseline_flag_rate = (flagged_count / len(baseline_df)) * 100\n",
    "\n",
    "        # Store the result, formatted as a percentage string\n",
    "        row[f'min_votes_{min_v}'] = f\"{baseline_flag_rate:.2f}%\"\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "# --- 4. Display the Results in a Clear Table ---\n",
    "calibration_results_df = pd.DataFrame(results).set_index('z_score_cutoff')\n",
    "\n",
    "print(\"\\n--- Calibration Results (Baseline Flag Rate %) ---\")\n",
    "print(\"This table shows the percentage of pre-ChatGPT revisions that would be flagged for each parameter combination.\")\n",
    "print(calibration_results_df)\n",
    "\n"
   ],
   "id": "e21f9f5eb95d9476",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating thresholds using 6118 baseline revisions (pre-2022-11-01).\n",
      "\n",
      "--- Calibration Results (Baseline Flag Rate %) ---\n",
      "This table shows the percentage of pre-ChatGPT revisions that would be flagged for each parameter combination.\n",
      "               min_votes_2 min_votes_3 min_votes_4 min_votes_5 min_votes_6\n",
      "z_score_cutoff                                                            \n",
      "2.0                 19.81%       7.45%       3.24%       1.63%       0.74%\n",
      "2.5                 10.95%       3.61%       1.81%       0.92%       0.33%\n",
      "3.0                  5.72%       2.35%       1.00%       0.29%       0.15%\n",
      "3.5                  3.60%       1.27%       0.51%       0.15%       0.13%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sanity Check\n",
    "(check if normalization was done properly)\n"
   ],
   "id": "6f5b19dbd09d4379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T13:26:37.950362Z",
     "start_time": "2025-08-30T13:26:37.868540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def verify_wiki_normalization(\n",
    "    normalized_df: pd.DataFrame,\n",
    "    raw_feature: str,\n",
    "    category_to_check: str,\n",
    "    category_col: str = 'root',\n",
    "    timestamp_col: str = 'timestamp',\n",
    "    baseline_end_date: str = '2022-11-01'\n",
    "):\n",
    "    \"\"\"\n",
    "    Verifies the per-category, time-based z-score calculation for a single\n",
    "    feature and a single category. This version uses a robust datetime check.\n",
    "\n",
    "    Args:\n",
    "        normalized_df (pd.DataFrame): The full DataFrame after normalization.\n",
    "        raw_feature (str): The name of the raw feature to check (e.g., 'fre').\n",
    "        category_to_check (str): The specific category to verify (e.g., 'History').\n",
    "        category_col (str): The name of the category column.\n",
    "        timestamp_col (str): The name of the timestamp column.\n",
    "        baseline_end_date (str): The date defining the end of the baseline period.\n",
    "    \"\"\"\n",
    "    zscore_feature = f\"{raw_feature}_zscore\"\n",
    "\n",
    "    print(f\"--- Verifying Z-Score for feature: '{raw_feature}' in Category: '{category_to_check}' ---\")\n",
    "\n",
    "    df = normalized_df.copy()\n",
    "\n",
    "    # --- CORRECTED DATETIME CHECK ---\n",
    "    # Use the pandas-native API for checking datetime types, which handles timezones correctly.\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
    "        print(f\"Converting '{timestamp_col}' to datetime...\")\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], utc=True, errors='coerce')\n",
    "\n",
    "    # 1. Manually isolate the baseline data for THIS SPECIFIC CATEGORY\n",
    "    baseline_df = df[\n",
    "        (df[category_col] == category_to_check) &\n",
    "        (df[timestamp_col] < pd.Timestamp(baseline_end_date, tz='UTC'))\n",
    "    ]\n",
    "\n",
    "    if len(baseline_df) < 2: # Need at least 2 points to calculate std dev\n",
    "        print(f\"Warning: Not enough baseline data ({len(baseline_df)} rows) found for '{category_to_check}' to verify. Skipping.\")\n",
    "        print(\"-\" * 70)\n",
    "        return\n",
    "\n",
    "    # 2. Manually calculate the mean and std dev for this category's baseline\n",
    "    baseline_mean = baseline_df[raw_feature].mean()\n",
    "    baseline_std = baseline_df[raw_feature].std()\n",
    "\n",
    "    print(f\"Manual Baseline Calculation for '{category_to_check}':\")\n",
    "    print(f\"  - Mean of '{raw_feature}': {baseline_mean:.6f}\")\n",
    "    print(f\"  - Std Dev of '{raw_feature}': {baseline_std:.6f}\\n\")\n",
    "\n",
    "    # 3. Manually compute the z-scores for ALL data in this category\n",
    "    category_df = df[df[category_col] == category_to_check]\n",
    "\n",
    "    if pd.notna(baseline_std) and baseline_std != 0:\n",
    "        manually_computed_zscores = (category_df[raw_feature] - baseline_mean) / baseline_std\n",
    "    else:\n",
    "        # If std is 0 or NaN, the result should be 0, matching the function's logic\n",
    "        manually_computed_zscores = pd.Series(0.0, index=category_df.index)\n",
    "\n",
    "    manually_computed_zscores = manually_computed_zscores.fillna(0)\n",
    "\n",
    "    # 4. Get the z-scores produced by your function for this category\n",
    "    function_zscores = category_df[zscore_feature].fillna(0)\n",
    "\n",
    "    # 5. Compare the two results\n",
    "    are_they_equal = np.allclose(manually_computed_zscores, function_zscores)\n",
    "\n",
    "    if are_they_equal:\n",
    "        print(f\"✅ SUCCESS: The function's z-scores for '{category_to_check}' match the manual calculation.\")\n",
    "    else:\n",
    "        print(f\"❌ FAILURE: The z-scores for '{category_to_check}' do not match.\")\n",
    "        diff_df = pd.DataFrame({\n",
    "            'function_z': function_zscores,\n",
    "            'manual_z': manually_computed_zscores,\n",
    "            'difference': function_zscores - manually_computed_zscores\n",
    "        })\n",
    "        print(\"Showing first 5 rows with significant differences:\")\n",
    "        display(diff_df[diff_df['difference'].abs() > 1e-9].head())\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "# Assumes 'final_wiki_df' is available in your notebook.\n",
    "\n",
    "# Verify for a major category\n",
    "verify_wiki_normalization(\n",
    "    normalized_df=final_df,\n",
    "    raw_feature='fre',\n",
    "    category_to_check='History'\n",
    ")\n",
    "\n",
    "# Verify for another one\n",
    "if 'Computing' in final_df['root'].unique():\n",
    "    verify_wiki_normalization(\n",
    "        normalized_df=final_df,\n",
    "        raw_feature='perplexity',\n",
    "        category_to_check='Computing'\n",
    "    )\n"
   ],
   "id": "4797e166a4ffa78d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Z-Score for feature: 'fre' in Category: 'History' ---\n",
      "Manual Baseline Calculation for 'History':\n",
      "  - Mean of 'fre': 27.680064\n",
      "  - Std Dev of 'fre': 19.238637\n",
      "\n",
      "✅ SUCCESS: The function's z-scores for 'History' match the manual calculation.\n",
      "----------------------------------------------------------------------\n",
      "--- Verifying Z-Score for feature: 'perplexity' in Category: 'Computing' ---\n",
      "Manual Baseline Calculation for 'Computing':\n",
      "  - Mean of 'perplexity': 485.400160\n",
      "  - Std Dev of 'perplexity': 276.798486\n",
      "\n",
      "✅ SUCCESS: The function's z-scores for 'Computing' match the manual calculation.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T13:26:22.249120Z",
     "start_time": "2025-08-30T13:26:22.242166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# unique values for root column\n",
    "final_df['root'].unique()"
   ],
   "id": "2c20f15345d50278",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['History', 'Politics', 'Technology', 'Computing', 'Biology',\n",
       "       'Chemistry', 'Physics', 'Film', 'Music', 'Science', 'Engineering',\n",
       "       'Elections', 'Political parties', 'Popular culture', 'Television',\n",
       "       'Political history', 'History by country', 'Military history',\n",
       "       'Medicine', 'Video games'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
