{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optimized Wikipedia Article Analysis Pipeline\n",
    "\n",
    "This notebook implements an optimized version of the Wikipedia article analysis pipeline with parallelization and improved error handling for better performance and stability with large datasets.\n"
   ],
   "id": "b644f91e31e37d85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:12.944840Z",
     "start_time": "2025-06-21T17:19:56.499504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from textstat import textstat\n",
    "import re\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "\n",
    "# Import our parallel processing utilities\n",
    "from parallel import (\n",
    "    process_batch_with_progress,\n",
    "    process_dataframe_parallel,\n",
    "    gpu_batch_process,\n",
    "    resilient_api_call,\n",
    "    CheckpointManager\n",
    ")\n"
   ],
   "id": "c55fd3341d7da417",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\envs\\wiki\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:14.146878Z",
     "start_time": "2025-06-21T17:20:13.517029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n"
   ],
   "id": "33b72d6b9405fb23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA version: 11.8\n",
      "CUDA available: True\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:16.252381Z",
     "start_time": "2025-06-21T17:20:14.169767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load GPT-2 model\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available! Moving model to GPU...\")\n",
    "    model.to(\"cuda\")\n",
    "else:\n",
    "    print('CUDA not available! Using CPU (this will be slow).')\n"
   ],
   "id": "ee782f6eea16988",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available! Moving model to GPU...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Wikipedia API Functions with Improved Resilience\n",
   "id": "66b9876d2caca305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:16.278845Z",
     "start_time": "2025-06-21T17:20:16.272935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_category_members(category, cmtype=\"page\", namespace=0, limit=500):\n",
    "    \"\"\"\n",
    "    Fetches members of a given Wikipedia category with improved error handling.\n",
    "\n",
    "    Args:\n",
    "        category: Category name without the 'Category:' prefix\n",
    "        cmtype: 'page', 'subcat', or 'file'\n",
    "        namespace: Namespace index (0 for articles)\n",
    "        limit: Number of results per request (max 500 for users)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'pageid' and 'title'\n",
    "    \"\"\"\n",
    "\n",
    "    def api_call(**params):\n",
    "        S = requests.Session()\n",
    "        res = S.get(\"https://en.wikipedia.org/w/api.php\", params=params, timeout=30)\n",
    "        res.raise_for_status()\n",
    "        return res.json()\n",
    "\n",
    "    members = []\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": f\"Category:{category}\",\n",
    "        \"cmtype\": cmtype,\n",
    "        \"cmlimit\": limit,\n",
    "        \"cmnamespace\": namespace,\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Use our resilient API call function with retries\n",
    "            data = resilient_api_call(api_call, max_retries=5, **params)\n",
    "            batch = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "            members.extend(batch)\n",
    "\n",
    "            if \"continue\" in data:\n",
    "                params.update(data[\"continue\"])\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching category members for {category}: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    return members\n"
   ],
   "id": "97f46b96d52eb61e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:16.316234Z",
     "start_time": "2025-06-21T17:20:16.306638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_articles_from_categories(categories, include_subcats=False, max_subcat_depth=1, checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    Given a list of category names, fetches all articles in them with checkpointing.\n",
    "    Optionally includes pages from subcategories up to specified depth.\n",
    "\n",
    "    Args:\n",
    "        categories: List of category names (strings without prefix)\n",
    "        include_subcats: Whether to traverse into subcategories\n",
    "        max_subcat_depth: Maximum depth for subcategory traversal\n",
    "        checkpoint_path: Path to save checkpoint data\n",
    "\n",
    "    Returns:\n",
    "        Set of page titles\n",
    "    \"\"\"\n",
    "    # Initialize checkpoint manager if path provided\n",
    "    checkpoint_mgr = None\n",
    "    if checkpoint_path:\n",
    "        checkpoint_mgr = CheckpointManager(checkpoint_path, save_interval=10)\n",
    "        # If we have completed results, return them\n",
    "        if checkpoint_mgr.processed_count == len(categories):\n",
    "            all_articles = set()\n",
    "            for result in checkpoint_mgr.get_results():\n",
    "                all_articles.update(result)\n",
    "            return all_articles\n",
    "\n",
    "    all_articles = set()\n",
    "    seen_cats = set()\n",
    "\n",
    "    def _recurse(cat, depth):\n",
    "        if cat in seen_cats or depth < 0:\n",
    "            return set()\n",
    "        seen_cats.add(cat)\n",
    "\n",
    "        cat_articles = set()\n",
    "\n",
    "        # Fetch pages\n",
    "        pages = get_category_members(cat, cmtype=\"page\")\n",
    "        for p in pages:\n",
    "            cat_articles.add(p[\"title\"])\n",
    "\n",
    "        if include_subcats and depth > 0:\n",
    "            subcats = get_category_members(cat, cmtype=\"subcat\", namespace=14)\n",
    "            for sc in subcats:\n",
    "                sc_name = sc[\"title\"].replace(\"Category:\", \"\")\n",
    "                subcat_articles = _recurse(sc_name, depth - 1)\n",
    "                cat_articles.update(subcat_articles)\n",
    "\n",
    "        return cat_articles\n",
    "\n",
    "    # Process categories in parallel\n",
    "    def process_category(cat):\n",
    "        try:\n",
    "            return _recurse(cat, max_subcat_depth)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing category {cat}: {str(e)}\")\n",
    "            return set()\n",
    "\n",
    "    # Get pending categories if using checkpoints\n",
    "    if checkpoint_mgr:\n",
    "        pending_indices = checkpoint_mgr.get_pending_indices(len(categories))\n",
    "        pending_categories = [categories[i] for i in pending_indices]\n",
    "        print(f\"Processing {len(pending_categories)} pending categories out of {len(categories)} total\")\n",
    "    else:\n",
    "        pending_categories = categories\n",
    "\n",
    "    # Process categories in parallel\n",
    "    results = process_batch_with_progress(\n",
    "        process_category,\n",
    "        pending_categories,\n",
    "        desc=\"Traversing categories\",\n",
    "        use_threads=True,  # Use threads for I/O-bound tasks\n",
    "        cpu_intensive=False\n",
    "    )\n",
    "\n",
    "    # Save results to checkpoint if using checkpoints\n",
    "    if checkpoint_mgr:\n",
    "        for i, result in enumerate(results):\n",
    "            idx = pending_indices[i] if pending_indices else i\n",
    "            checkpoint_mgr.add_result(idx, result)\n",
    "\n",
    "        # Get all results including previously checkpointed ones\n",
    "        all_results = checkpoint_mgr.get_results()\n",
    "    else:\n",
    "        all_results = results\n",
    "\n",
    "    # Combine all article sets\n",
    "    for article_set in all_results:\n",
    "        if article_set:  # Skip None results from errors\n",
    "            all_articles.update(article_set)\n",
    "\n",
    "    return all_articles\n"
   ],
   "id": "13f9e281a9e26c16",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:20:16.349927Z",
     "start_time": "2025-06-21T17:20:16.334110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_revision_snapshots(\n",
    "        title: str,\n",
    "        start_ts: str,\n",
    "        end_ts: str,\n",
    "        freq: str = \"7D\",\n",
    "        bot_test_fn: callable = None,\n",
    "        carry_forward: bool = True,\n",
    "        max_retries: int = 5,\n",
    "        backoff_factor: float = 1.0,\n",
    "        checkpoint_path: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches all revisions (including full content) for `title` between start_ts and end_ts,\n",
    "    then returns snapshots at given frequency with content included.\n",
    "\n",
    "    This version includes improved error handling and checkpointing.\n",
    "\n",
    "    Args:\n",
    "        title: Wikipedia page title\n",
    "        start_ts: ISO8601 timestamp string, e.g. \"2020-01-01T00:00:00Z\"\n",
    "        end_ts: ISO8601 timestamp string, e.g. \"2020-12-31T23:59:59Z\"\n",
    "        freq: pandas offset alias (e.g. \"7D\")\n",
    "        bot_test_fn: callable to flag bots (user->bool)\n",
    "        carry_forward: reuse last snapshot if no newer revision\n",
    "        max_retries: number of retries on network failure\n",
    "        backoff_factor: multiplier for retry backoff in seconds\n",
    "        checkpoint_path: path to pickle intermediate results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns ['page_title','snapshot_ts','rev_id','timestamp',\n",
    "        'user','is_bot','content']\n",
    "    \"\"\"\n",
    "    # Check if we have a checkpoint\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            return pd.read_pickle(checkpoint_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "\n",
    "    def api_call(**params):\n",
    "        session = requests.Session()\n",
    "        resp = session.get(\"https://en.wikipedia.org/w/api.php\", params=params, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    # Phase 1: Fetch metadata + content in one pass\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"ids|timestamp|user|content\",\n",
    "        \"rvstart\": end_ts,\n",
    "        \"rvend\": start_ts,\n",
    "        \"rvlimit\": \"max\",\n",
    "        \"titles\": title,\n",
    "        \"redirects\": 1,\n",
    "        \"rvslots\": \"main\",\n",
    "    }\n",
    "\n",
    "    all_revs = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Use our resilient API call function\n",
    "            data = resilient_api_call(\n",
    "                api_call,\n",
    "                max_retries=max_retries,\n",
    "                initial_backoff=backoff_factor,\n",
    "                **params\n",
    "            )\n",
    "\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            for page in pages.values():\n",
    "                for rev in page.get(\"revisions\", []) or []:\n",
    "                    ts_str = rev.get(\"timestamp\")\n",
    "                    if not ts_str:\n",
    "                        continue\n",
    "                    ts = parser.isoparse(ts_str)\n",
    "                    user = rev.get(\"user\", \"\")\n",
    "                    # content slot\n",
    "                    content = rev.get(\"slots\", {}).get(\"main\", {}).get(\"*\", \"\")\n",
    "                    all_revs.append({\n",
    "                        \"rev_id\": rev.get(\"revid\"),\n",
    "                        \"timestamp\": ts,\n",
    "                        \"user\": user,\n",
    "                        \"is_bot\": bot_test_fn(user) if bot_test_fn else False,\n",
    "                        \"content\": content,\n",
    "                    })\n",
    "\n",
    "            if \"continue\" in data:\n",
    "                params.update(data[\"continue\"])\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching revisions for {title}: {str(e)}\")\n",
    "        # If we have some revisions, continue with what we have\n",
    "        if not all_revs:\n",
    "            return pd.DataFrame(columns=[\"page_title\", \"snapshot_ts\", \"rev_id\",\n",
    "                                         \"timestamp\", \"user\", \"is_bot\", \"content\"])\n",
    "\n",
    "    # If no revisions, return empty DataFrame\n",
    "    if not all_revs:\n",
    "        return pd.DataFrame(columns=[\"page_title\", \"snapshot_ts\", \"rev_id\",\n",
    "                                     \"timestamp\", \"user\", \"is_bot\", \"content\"])\n",
    "\n",
    "    # Build DataFrame and sort\n",
    "    df = pd.DataFrame(all_revs)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Phase 2: Snapshot selection\n",
    "    timestamps = df[\"timestamp\"]\n",
    "    sample_times = pd.date_range(\n",
    "        start=pd.to_datetime(start_ts),\n",
    "        end=pd.to_datetime(end_ts),\n",
    "        freq=freq,\n",
    "        tz=timestamps.dt.tz\n",
    "    )\n",
    "\n",
    "    snaps = []\n",
    "    last_snap_time = None\n",
    "\n",
    "    for snap_t in sample_times:\n",
    "        pos = timestamps.searchsorted(snap_t, side='right') - 1\n",
    "        if pos < 0:\n",
    "            continue\n",
    "        rev_time = timestamps.iloc[pos]\n",
    "        # skip if no carry_forward and no new revision\n",
    "        if not carry_forward and last_snap_time is not None and rev_time <= last_snap_time:\n",
    "            last_snap_time = snap_t\n",
    "            continue\n",
    "        row = df.iloc[pos].to_dict()\n",
    "        row[\"snapshot_ts\"] = snap_t\n",
    "        row[\"page_title\"] = title\n",
    "        snaps.append(row)\n",
    "        last_snap_time = snap_t\n",
    "\n",
    "    df_snap = pd.DataFrame(snaps)\n",
    "    cols = [\"page_title\", \"snapshot_ts\", \"rev_id\", \"timestamp\", \"user\", \"is_bot\", \"content\"]\n",
    "    result_df = df_snap[cols] if not df_snap.empty else pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Save checkpoint if path provided\n",
    "    if checkpoint_path and not result_df.empty:\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            result_df.to_pickle(checkpoint_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "    return result_df\n"
   ],
   "id": "ee6439093cb54df0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Fetch Articles from Categories\n",
   "id": "8d69a148cff467ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:22:18.786422Z",
     "start_time": "2025-06-21T17:20:16.370364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modify parallel processing configuration\n",
    "from parallel import process_batch_with_progress\n",
    "\n",
    "# Define categories to fetch\n",
    "categories = [\n",
    "    \"Politics\",\n",
    "    \"History\"\n",
    "]\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Configure more conservative parallel processing settings\n",
    "PARALLEL_CONFIG = {\n",
    "    'max_workers': 4,  # Reduce number of concurrent workers\n",
    "    'use_threads': True,  # Use threads instead of processes for API calls\n",
    "    'cpu_intensive': False,\n",
    "    'timeout': 300,  # 5 minutes timeout\n",
    "    'chunk_size': 5  # Process in smaller chunks\n",
    "}\n",
    "\n",
    "\n",
    "# Fetch articles with improved error handling\n",
    "def fetch_with_retries(category, **kwargs):\n",
    "    try:\n",
    "        results = fetch_articles_from_categories(\n",
    "            [category],\n",
    "            include_subcats=True,\n",
    "            max_subcat_depth=2\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing category {category}: {str(e)}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "# Process categories with more robust error handling\n",
    "articles = set()\n",
    "results = process_batch_with_progress(\n",
    "    fetch_with_retries,\n",
    "    categories,\n",
    "    desc=\"Processing categories\",\n",
    "    **PARALLEL_CONFIG\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "for result in results:\n",
    "    if result:\n",
    "        articles.update(result)\n",
    "\n",
    "print(f\"Fetched {len(articles)} articles from categories and subcategories.\")\n",
    "\n",
    "# Save to file with error handling\n",
    "try:\n",
    "    with open(\"article_names_list.pkl\", \"wb\") as f:\n",
    "        pickle.dump(articles, f)\n",
    "    print(\"Successfully saved articles list\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving articles list: {str(e)}\")\n"
   ],
   "id": "37799a352ee70630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 items with 4 workers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing categories:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "875e191d00d04a4c8108f06abec2fd35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 items with 4 workers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a687f6869174d61bdf1a96adb5c5a1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 items with 4 workers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Traversing categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22e9c5add06e42c6b6676ab998942bb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 29306 articles from categories and subcategories.\n",
      "Successfully saved articles list\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:22:18.854952Z",
     "start_time": "2025-06-21T17:22:18.843259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the articles from the saved file if needed\n",
    "try:\n",
    "    with open(\"article_names_list.pkl\", \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "    print(f\"Loaded {len(articles)} articles from file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Article list file not found.\")\n"
   ],
   "id": "29ead022b98043d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29306 articles from file.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Fetch Revision Snapshots in Parallel\n",
   "id": "2107e90af0c2402c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:22:18.906366Z",
     "start_time": "2025-06-21T17:22:18.902405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define time range\n",
    "START_TIMESTAMP = \"2022-01-01T00:00:00Z\"\n",
    "END_TIMESTAMP = \"2024-01-31T23:59:59Z\"\n",
    "\n",
    "# Define columns for the DataFrame\n",
    "columns = [\n",
    "    \"page_title\", \"rev_id\", \"timestamp\", \"user\", \"is_bot\", \"content\"\n",
    "]\n",
    "\n",
    "\n",
    "# Function to check if a username is a bot\n",
    "def is_bot_username(username: str) -> bool:\n",
    "    return username.lower().endswith(\"bot\")\n"
   ],
   "id": "980a950501fd1895",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:23:30.405528Z",
     "start_time": "2025-06-21T17:23:30.398525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to fetch revisions for a single article with checkpointing\n",
    "def fetch_article_revisions(article, **kwargs):\n",
    "    checkpoint_path = f\"checkpoints/revisions/{article.replace('/', '_')}.pkl\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        revs = fetch_revision_snapshots(\n",
    "            article,\n",
    "            START_TIMESTAMP,\n",
    "            END_TIMESTAMP,\n",
    "            freq=\"1ME\",\n",
    "            bot_test_fn=is_bot_username,\n",
    "            carry_forward=False,\n",
    "            checkpoint_path=checkpoint_path\n",
    "        )\n",
    "        return revs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching revisions for {article}: {str(e)}\")\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "# Limit to a sample for testing\n",
    "sample_size = 100  # Adjust as needed\n",
    "sample_pages = list(articles)[:sample_size]\n",
    "print(f\"Processing {len(sample_pages)} sample pages\")\n"
   ],
   "id": "51e89b9a13850ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 sample pages\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:23:34.165281Z",
     "start_time": "2025-06-21T17:23:34.075986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modified checkpoint handling\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    \"checkpoints/article_processing.pkl\",\n",
    "    save_interval=2  # Save more frequently\n",
    ")\n",
    "\n",
    "# Get pending articles with error handling\n",
    "try:\n",
    "    pending_indices = checkpoint_mgr.get_pending_indices(len(sample_pages))\n",
    "    pending_articles = [sample_pages[i] for i in pending_indices]\n",
    "    print(f\"Processing {len(pending_articles)} pending articles out of {len(sample_pages)} total\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting pending articles: {str(e)}\")\n",
    "    pending_articles = sample_pages\n",
    "    pending_indices = list(range(len(sample_pages)))\n",
    "\n",
    "# Process pending articles with improved error handling\n",
    "if pending_articles:\n",
    "    results = process_batch_with_progress(\n",
    "        fetch_article_revisions,\n",
    "        pending_articles,\n",
    "        desc=\"Fetching revisions\",\n",
    "        max_workers=3,  # Reduce concurrent workers\n",
    "        use_threads=True,\n",
    "        cpu_intensive=False,\n",
    "        batch_size=5  # Process in smaller batches\n",
    "    )\n",
    "\n",
    "    # Save results with error handling\n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            if result is not None:\n",
    "                idx = pending_indices[i]\n",
    "                checkpoint_mgr.add_result(idx, result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving result {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Combine results with error handling\n",
    "try:\n",
    "    all_results = checkpoint_mgr.get_results()\n",
    "    all_dfs = [df for df in all_results if df is not None and not df.empty]\n",
    "\n",
    "    if all_dfs:\n",
    "        tiny_revs = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Combined DataFrame has {len(tiny_revs)} rows\")\n",
    "    else:\n",
    "        columns = [\"page_title\", \"snapshot_ts\", \"rev_id\", \"timestamp\", \"user\", \"is_bot\", \"content\"]\n",
    "        tiny_revs = pd.DataFrame(columns=columns)\n",
    "        print(\"No revisions found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining results: {str(e)}\")\n"
   ],
   "id": "c5d50a3ced844b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint with 100 processed items\n",
      "Processing 0 pending articles out of 100 total\n",
      "Combined DataFrame has 850 rows\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:27:05.652671Z",
     "start_time": "2025-06-21T17:27:05.611781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the DataFrame\n",
    "tiny_revs.to_pickle(\"mini_history_politics_100_articles_every1m_before_cleaning.pkl\")\n",
    "tiny_revs.to_csv(\"mini_history_politics_100_articles_every1m_before_cleaning.csv\", index=False)\n",
    "\n",
    "# Display sample\n",
    "tiny_revs.head()\n"
   ],
   "id": "6de08222c4fe613f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tiny_revs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Save the DataFrame\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m tiny_revs.to_pickle(\u001B[33m\"\u001B[39m\u001B[33mmini_history_politics_100_articles_every1m_before_cleaning.pkl\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m tiny_revs.to_csv(\u001B[33m\"\u001B[39m\u001B[33mmini_history_politics_100_articles_every1m_before_cleaning.csv\u001B[39m\u001B[33m\"\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Display sample\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'tiny_revs' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Text Cleaning & Sentence/Token Parsing\n",
   "id": "d48340e2053dfa5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:27:17.154265Z",
     "start_time": "2025-06-21T17:27:17.149855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(wikitext: str) -> str:\n",
    "    \"\"\"Clean Wikipedia markup text to plain text\"\"\"\n",
    "    if not isinstance(wikitext, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove wiki markupâ€”keep plain text for prototype\n",
    "    text = re.sub(r\"<ref>.*?</ref>\", \"\", wikitext, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text)  # simple template removal\n",
    "    text = re.sub(r\"\\[\\[([^|\\]]*\\|)?([^\\]]+)\\]\\]\", r\"\\2\", text)  # keep link text\n",
    "    text = re.sub(r\"''+\", \"\", text)  # remove italic/bold\n",
    "    # Remove non-alphabetic chars except basic punctuation\n",
    "    text = re.sub(r\"[^A-Za-z0-9 \\.\\,\\!\\?\\-\\'\\\"]+ \", \" \", text)\n",
    "    # Lowercase and collapse whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ],
   "id": "aa91783347db8245",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5d3a8297d35377"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:31:49.941057Z",
     "start_time": "2025-06-21T17:31:49.912899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean text in parallel\n",
    "tiny_revs = process_dataframe_parallel(\n",
    "    tiny_revs,\n",
    "    clean_text,\n",
    "    column=\"content\",\n",
    "    new_column=\"plain_text\",\n",
    "    use_threads=True,  # Text cleaning is I/O-bound\n",
    "    cpu_intensive=False\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "tiny_revs[[\"page_title\", \"plain_text\"]].head()\n"
   ],
   "id": "b2baa99f28bae4bd",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_dataframe_parallel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Clean text in parallel\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m tiny_revs = process_dataframe_parallel(\n\u001B[32m      3\u001B[39m     tiny_revs,\n\u001B[32m      4\u001B[39m     clean_text,\n\u001B[32m      5\u001B[39m     column=\u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      6\u001B[39m     new_column=\u001B[33m\"\u001B[39m\u001B[33mplain_text\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     use_threads=\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# Text cleaning is I/O-bound\u001B[39;00m\n\u001B[32m      8\u001B[39m     cpu_intensive=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# Display sample\u001B[39;00m\n\u001B[32m     12\u001B[39m tiny_revs[[\u001B[33m\"\u001B[39m\u001B[33mpage_title\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mplain_text\u001B[39m\u001B[33m\"\u001B[39m]].head()\n",
      "\u001B[31mNameError\u001B[39m: name 'process_dataframe_parallel' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:24:28.143817Z",
     "start_time": "2025-06-21T17:24:28.134686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def get_spacy_model():\n",
    "    # This attribute is specific to each process\n",
    "    if not hasattr(get_spacy_model, \"nlp\"):\n",
    "        get_spacy_model.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    return get_spacy_model.nlp\n",
    "\n",
    "\n",
    "def parse_with_spacy(text: str):\n",
    "    \"\"\"Parse text with spaCy and extract linguistic features (multiprocessing safe).\"\"\"\n",
    "    nlp = get_spacy_model()  # <- Load per worker!\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\n",
    "            \"upos_props\": {},\n",
    "            \"mean_dep_depth\": 0,\n",
    "            \"clause_ratio\": 0,\n",
    "            \"voice_ratio\": 0,\n",
    "            \"sentences\": [],\n",
    "            \"tokens\": []\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # Limit text length to prevent memory issues\n",
    "        max_length = 100000  # Adjust based on available memory\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "\n",
    "        doc = nlp(text)\n",
    "        total_tokens = len(doc)\n",
    "\n",
    "        if total_tokens == 0:\n",
    "            return {\n",
    "                \"upos_props\": {},\n",
    "                \"mean_dep_depth\": 0,\n",
    "                \"clause_ratio\": 0,\n",
    "                \"voice_ratio\": 0,\n",
    "                \"sentences\": [],\n",
    "                \"tokens\": []\n",
    "            }\n",
    "\n",
    "        # POS proportions\n",
    "        pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "        upos_props = {doc.vocab[pos].text: cnt / total_tokens for pos, cnt in pos_counts.items()}\n",
    "\n",
    "        # Dependency depth approximation\n",
    "        def token_depth(token):\n",
    "            depth = 0\n",
    "            while token != token.head:\n",
    "                token = token.head\n",
    "                depth += 1\n",
    "            return depth\n",
    "\n",
    "        depths = [token_depth(token) for token in doc]\n",
    "        mean_depth = sum(depths) / total_tokens if total_tokens else 0\n",
    "\n",
    "        # Clause ratio\n",
    "        clause_tags = sum(1 for token in doc if token.dep_ in (\"advcl\", \"ccomp\", \"xcomp\"))\n",
    "        clause_ratio = clause_tags / (len(list(doc.sents)) or 1)\n",
    "\n",
    "        # Passive voice ratio\n",
    "        passive_count = sum(1 for token in doc if token.dep_ == \"auxpass\")\n",
    "        voice_ratio = (total_tokens - passive_count) / (total_tokens or 1)\n",
    "\n",
    "        return {\n",
    "            \"upos_props\": upos_props,\n",
    "            \"mean_dep_depth\": mean_depth,\n",
    "            \"clause_ratio\": clause_ratio,\n",
    "            \"voice_ratio\": voice_ratio,\n",
    "            \"sentences\": [sent.text for sent in doc.sents],\n",
    "            \"tokens\": [token.text for token in doc]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing text: {str(e)}\")\n",
    "        return {\n",
    "            \"upos_props\": {},\n",
    "            \"mean_dep_depth\": 0,\n",
    "            \"clause_ratio\": 0,\n",
    "            \"voice_ratio\": 0,\n",
    "            \"sentences\": [],\n",
    "            \"tokens\": []\n",
    "        }\n"
   ],
   "id": "464c74f8bfff46b6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:26:57.151866Z",
     "start_time": "2025-06-21T17:26:56.448458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test single call\n",
    "parse_with_spacy(\"This is a test sentence. It is used for debugging purposes.\")\n",
    "\n",
    "# Test with parallel processing on a tiny sample (e.g., 5 rows)\n",
    "tiny_revs_sample = tiny_revs.head(5)\n",
    "# tiny_revs_sample = tiny_revs\n",
    "result = process_dataframe_parallel(\n",
    "    tiny_revs_sample,\n",
    "    parse_with_spacy,\n",
    "    column=\"plain_text\",\n",
    "    new_column=\"parsed\",\n",
    "    use_threads=True,\n",
    "    cpu_intensive=False\n",
    "\n",
    ")\n",
    "result.head()\n"
   ],
   "id": "d80d311cb02921fc",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_with_spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Test single call\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m parse_with_spacy(\u001B[33m\"\u001B[39m\u001B[33mThis is a test sentence. It is used for debugging purposes.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Test with parallel processing on a tiny sample (e.g., 5 rows)\u001B[39;00m\n\u001B[32m      5\u001B[39m tiny_revs_sample = tiny_revs.head(\u001B[32m5\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'parse_with_spacy' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Feature Extraction Functions\n",
   "id": "7a396d55e784cf96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:05:39.403229Z",
     "start_time": "2025-06-21T17:05:39.389933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load trigger words\n",
    "trigger_file = \"combined_chatgpt_words.csv\"\n",
    "tiny_trigger = set(pd.read_csv(trigger_file, header=None)[0].tolist())\n",
    "\n",
    "\n",
    "def compute_delta(text: str, trigger_set: set, baseline_freq: float = 0.0001):\n",
    "    \"\"\"Compute delta between observed and baseline frequency of trigger words\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    freq = sum(1 for t in tokens if t in trigger_set) / len(tokens)\n",
    "    return freq - baseline_freq\n"
   ],
   "id": "a8f225c9a5dfd8c9",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:05:42.811146Z",
     "start_time": "2025-06-21T17:05:42.171114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute delta in parallel\n",
    "tiny_revs = process_dataframe_parallel(\n",
    "    tiny_revs,\n",
    "    compute_delta,\n",
    "    column=\"plain_text\",\n",
    "    new_column=\"delta\",\n",
    "    use_threads=True,  # This is lightweight\n",
    "    trigger_set=tiny_trigger\n",
    ")\n"
   ],
   "id": "11e33050aa7b67c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing plain_text:   0%|          | 0/850 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5eb9f67990ca4ee4ab3c9e8533513fe2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:05:46.640859Z",
     "start_time": "2025-06-21T17:05:46.623429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_perplexity_and_burstiness(text: str, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    GPU-optimized perplexity calculation with input validation for GPT-2\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"WARNING: CUDA not available!\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    if model.device.type != \"cuda\":\n",
    "        model.to(\"cuda\")\n",
    "\n",
    "    try:\n",
    "        encodings = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        input_ids = encodings.input_ids\n",
    "\n",
    "        # Clamp input_ids to valid range\n",
    "        vocab_size = model.config.vocab_size\n",
    "        if torch.any(input_ids >= vocab_size) or torch.any(input_ids < 0):\n",
    "            input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n",
    "\n",
    "        if input_ids.shape[1] < 5:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        input_ids = input_ids.to(\"cuda\")\n",
    "\n",
    "        chunk_size = 8\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for i in range(0, input_ids.shape[1], chunk_size):\n",
    "            end_idx = min(i + chunk_size, input_ids.shape[1])\n",
    "            chunk = input_ids[:, i:end_idx]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(chunk, labels=chunk)\n",
    "                chunk_loss = outputs.loss.item() * chunk.shape[1]\n",
    "                total_loss += chunk_loss\n",
    "                total_tokens += chunk.shape[1]\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping chunk {i}:{end_idx} due to error: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if total_tokens == 0:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "        log_probs = []\n",
    "        positions = [min(10, input_ids.shape[1] - 1), min(20, input_ids.shape[1] - 1)]\n",
    "        for pos in positions:\n",
    "            if pos < 5:\n",
    "                continue\n",
    "            try:\n",
    "                segment = input_ids[:, :pos]\n",
    "                with torch.no_grad():\n",
    "                    out = model(segment, labels=segment)\n",
    "                log_probs.append(-out.loss.item())\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        burstiness = float(pd.Series(log_probs).std()) if len(log_probs) > 1 else 0.0\n",
    "        return ppl, burstiness\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:100]}...\")\n",
    "        return 0.0, 0.0\n"
   ],
   "id": "a14974b1cea4c98a",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T17:08:54.326385Z",
     "start_time": "2025-06-21T17:07:29.275696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Process perplexity and burstiness in batches using GPU with progress bar\n",
    "texts = tiny_revs[\"plain_text\"].tolist()\n",
    "batch_size = 8\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing perplexity and burstiness\"):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    batch_results = [compute_perplexity_and_burstiness(text) for text in batch]\n",
    "    results.extend(batch_results)\n",
    "\n",
    "# Add results to DataFrame\n",
    "perplexity_values = [r[0] if r is not None else 0.0 for r in results]\n",
    "burstiness_values = [r[1] if r is not None else 0.0 for r in results]\n",
    "tiny_revs[\"perplexity\"] = perplexity_values\n",
    "tiny_revs[\"burstiness\"] = burstiness_values"
   ],
   "id": "2f551d806407ef87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Computing perplexity and burstiness:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31bfc6eeb5dc4d18840decf60fe5a816"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[88]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(texts), batch_size), desc=\u001B[33m\"\u001B[39m\u001B[33mComputing perplexity and burstiness\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     10\u001B[39m     batch = texts[i:i+batch_size]\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     batch_results = [compute_perplexity_and_burstiness(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[32m     12\u001B[39m     results.extend(batch_results)\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Add results to DataFrame\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[86]\u001B[39m\u001B[32m, line 46\u001B[39m, in \u001B[36mcompute_perplexity_and_burstiness\u001B[39m\u001B[34m(text, max_length)\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m         outputs = model(chunk, labels=chunk)\n\u001B[32m     47\u001B[39m     chunk_loss = outputs.loss.item() * chunk.shape[\u001B[32m1\u001B[39m]\n\u001B[32m     48\u001B[39m     total_loss += chunk_loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1210\u001B[39m, in \u001B[36mGPT2LMHeadModel.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m   1190\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1191\u001B[39m \u001B[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001B[39;00m\n\u001B[32m   1192\u001B[39m \u001B[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1206\u001B[39m \u001B[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[32m   1207\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1208\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1210\u001B[39m transformer_outputs = \u001B[38;5;28mself\u001B[39m.transformer(\n\u001B[32m   1211\u001B[39m     input_ids,\n\u001B[32m   1212\u001B[39m     past_key_values=past_key_values,\n\u001B[32m   1213\u001B[39m     attention_mask=attention_mask,\n\u001B[32m   1214\u001B[39m     cache_position=cache_position,\n\u001B[32m   1215\u001B[39m     token_type_ids=token_type_ids,\n\u001B[32m   1216\u001B[39m     position_ids=position_ids,\n\u001B[32m   1217\u001B[39m     head_mask=head_mask,\n\u001B[32m   1218\u001B[39m     inputs_embeds=inputs_embeds,\n\u001B[32m   1219\u001B[39m     encoder_hidden_states=encoder_hidden_states,\n\u001B[32m   1220\u001B[39m     encoder_attention_mask=encoder_attention_mask,\n\u001B[32m   1221\u001B[39m     use_cache=use_cache,\n\u001B[32m   1222\u001B[39m     output_attentions=output_attentions,\n\u001B[32m   1223\u001B[39m     output_hidden_states=output_hidden_states,\n\u001B[32m   1224\u001B[39m     return_dict=return_dict,\n\u001B[32m   1225\u001B[39m )\n\u001B[32m   1226\u001B[39m hidden_states = transformer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1228\u001B[39m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:939\u001B[39m, in \u001B[36mGPT2Model.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m    926\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    927\u001B[39m         block.\u001B[34m__call__\u001B[39m,\n\u001B[32m    928\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    936\u001B[39m         output_attentions,\n\u001B[32m    937\u001B[39m     )\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m939\u001B[39m     outputs = block(\n\u001B[32m    940\u001B[39m         hidden_states,\n\u001B[32m    941\u001B[39m         past_key_value=past_key_values,\n\u001B[32m    942\u001B[39m         cache_position=cache_position,\n\u001B[32m    943\u001B[39m         attention_mask=causal_mask,\n\u001B[32m    944\u001B[39m         head_mask=head_mask[i],\n\u001B[32m    945\u001B[39m         encoder_hidden_states=encoder_hidden_states,\n\u001B[32m    946\u001B[39m         encoder_attention_mask=encoder_attention_mask,\n\u001B[32m    947\u001B[39m         use_cache=use_cache,\n\u001B[32m    948\u001B[39m         output_attentions=output_attentions,\n\u001B[32m    949\u001B[39m         **kwargs,\n\u001B[32m    950\u001B[39m     )\n\u001B[32m    952\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    954\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:438\u001B[39m, in \u001B[36mGPT2Block.forward\u001B[39m\u001B[34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001B[39m\n\u001B[32m    435\u001B[39m     hidden_states = residual + cross_attn_output\n\u001B[32m    437\u001B[39m residual = hidden_states\n\u001B[32m--> \u001B[39m\u001B[32m438\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.ln_2(hidden_states)\n\u001B[32m    439\u001B[39m feed_forward_hidden_states = \u001B[38;5;28mself\u001B[39m.mlp(hidden_states)\n\u001B[32m    440\u001B[39m \u001B[38;5;66;03m# residual connection\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\wiki\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927\u001B[39m, in \u001B[36mModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1922\u001B[39m         \u001B[38;5;28mself\u001B[39m._backward_pre_hooks = OrderedDict()\n\u001B[32m   1924\u001B[39m \u001B[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001B[39;00m\n\u001B[32m   1925\u001B[39m \u001B[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001B[39;00m\n\u001B[32m   1926\u001B[39m \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1927\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) -> Union[Tensor, \u001B[33m\"\u001B[39m\u001B[33mModule\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1928\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m_parameters\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m   1929\u001B[39m         _parameters = \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m[\u001B[33m\"\u001B[39m\u001B[33m_parameters\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract attributes from parsed dictionaries\n",
    "tiny_revs[\"upos_props\"] = tiny_revs[\"parsed\"].apply(lambda x: x.get(\"upos_props\", {}))\n",
    "tiny_revs[\"mean_dep_depth\"] = tiny_revs[\"parsed\"].apply(lambda x: x.get(\"mean_dep_depth\", 0))\n",
    "tiny_revs[\"clause_ratio\"] = tiny_revs[\"parsed\"].apply(lambda x: x.get(\"clause_ratio\", 0))\n",
    "tiny_revs[\"voice_ratio\"] = tiny_revs[\"parsed\"].apply(lambda x: x.get(\"voice_ratio\", 0))\n"
   ],
   "id": "82b2d72f73c499fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_readability(text: str):\n",
    "    \"\"\"Compute readability metrics for text\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    try:\n",
    "        fre = textstat.flesch_reading_ease(text)\n",
    "        fog = textstat.gunning_fog(text)\n",
    "\n",
    "        # Characters per sentence\n",
    "        sentences = list(nlp(text).sents)\n",
    "        chars_per_sent = sum(len(sent.text) for sent in sentences) / (len(sentences) or 1)\n",
    "\n",
    "        # Sentences per paragraph (since we have flattened text, treat the entire text as one paragraph)\n",
    "        sents_per_para = len(sentences)  # toy assumption: 1 paragraph = all sentences\n",
    "\n",
    "        return fre, fog, chars_per_sent, sents_per_para\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing readability: {str(e)}\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n"
   ],
   "id": "aecc84832e8e7238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute readability metrics in parallel\n",
    "tiny_revs = process_dataframe_parallel(\n",
    "    tiny_revs,\n",
    "    compute_readability,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"fre\", \"fog\", \"chars_per_sent\", \"sents_per_para\"],\n",
    "    use_threads=False,  # CPU-intensive\n",
    "    cpu_intensive=True\n",
    ")\n"
   ],
   "id": "9626a85fba20fd20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_vocab_diversity(text: str, window_size: int = 250):\n",
    "    \"\"\"Compute vocabulary diversity metrics\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    tokens = text.split()[:window_size]\n",
    "    unique_count = len(set(tokens))\n",
    "    total_count = len(tokens) or 1\n",
    "\n",
    "    # Normalized TTR = unique / sqrt(2 * total)\n",
    "    nTTR = unique_count / ((2 * total_count) ** 0.5)\n",
    "\n",
    "    # Word-density: lines = count of '\\n' + 1, avg_line_len:\n",
    "    lines = text.count(\"\\n\") + 1\n",
    "    avg_line_len = sum(len(line) for line in text.split(\"\\n\")) / lines\n",
    "    wd = 100 * unique_count / (lines * (avg_line_len or 1))\n",
    "\n",
    "    return nTTR, wd\n"
   ],
   "id": "996fd2356c86add2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute vocabulary diversity in parallel\n",
    "tiny_revs = process_dataframe_parallel(\n",
    "    tiny_revs,\n",
    "    compute_vocab_diversity,\n",
    "    column=\"plain_text\",\n",
    "    new_column=[\"nTTR\", \"word_density\"],\n",
    "    use_threads=True  # This is lightweight\n",
    ")\n"
   ],
   "id": "75c8712aa420ea48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_line_length(text: str):\n",
    "    \"\"\"Compute average line length\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    avg_len = sum(len(line) for line in lines) / (len(lines) or 1)\n",
    "    return avg_len\n",
    "\n",
    "\n",
    "# Compute line length\n",
    "tiny_revs[\"avg_line_len\"] = tiny_revs[\"plain_text\"].apply(compute_line_length)\n"
   ],
   "id": "4790f05bbd9ba9e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_citation_delta(wikitext: str):\n",
    "    \"\"\"Compute citation delta\"\"\"\n",
    "    if not isinstance(wikitext, str) or not wikitext.strip():\n",
    "        return 0.0\n",
    "\n",
    "    # Count <ref> tags in raw wikitext\n",
    "    added = len(re.findall(r\"<ref[^>]*>\", wikitext))\n",
    "    removed = 0  # For prototype, assume no diff stored; set removed = 0\n",
    "    tokens_changed = len(wikitext.split()) or 1\n",
    "    return (added - removed) / tokens_changed\n",
    "\n",
    "\n",
    "# Compute citation delta\n",
    "tiny_revs[\"citation_delta\"] = tiny_revs[\"content\"].apply(compute_\n"
   ],
   "id": "af203fddd6e22ed6"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
