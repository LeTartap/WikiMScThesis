{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.219156Z",
     "start_time": "2025-05-05T19:13:52.112599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 1 – Imports & notebook niceties\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2            # reload local modules when you edit them\n",
    "# %config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import sys, json, time, random, itertools, pathlib, logging, textwrap, re, typing as T\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s │ %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n"
   ],
   "id": "b1731694f49e7f19",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.331982Z",
     "start_time": "2025-05-05T19:13:52.224341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2 – All user-tunable knobs in one place\n",
    "@dataclass\n",
    "class Config:\n",
    "    # --- INPUT -------------------------------------------------------------\n",
    "    fetch_pages: list[str] | None = None         # list of Wikipedia page titles\n",
    "    fetch_start: str | None = None               # \"YYYY-MM-DD\"\n",
    "    fetch_end: str | None = None\n",
    "    fetch_revs_per_page: int | None = 30         # None ⇒ pull ALL\n",
    "    fetch_delay: float = 0.1                     # seconds between API hits\n",
    "\n",
    "    file_paths: list[str] | None = None          # alternative: local JSONL(s)\n",
    "    text_field: str = \"added_text\"\n",
    "    timestamp_field: str = \"timestamp\"\n",
    "\n",
    "    # --- SAMPLING ----------------------------------------------------------\n",
    "    sample_size_revisions: int | None = 10_000   # None = keep everything\n",
    "    sample_seed: int = 42\n",
    "\n",
    "    # --- BASELINE WINDOW ---------------------------------------------------\n",
    "    baseline_start: str = \"2020-01\"              # inclusive YYYY-MM\n",
    "    baseline_end:   str = \"2022-11\"\n",
    "\n",
    "    # --- SPIKE THRESHOLDS --------------------------------------------------\n",
    "    gap_min: float = 0.5                         # per-million tokens\n",
    "    ratio_min: float = 10.0\n",
    "\n",
    "    # --- MISC --------------------------------------------------------------\n",
    "    verbose: bool = True\n"
   ],
   "id": "5e9c498c77f513a1",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.453717Z",
     "start_time": "2025-05-05T19:13:52.340825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 3 – Fetch ≤ N revisions/page via MediaWiki API, yield dicts\n",
    "\n",
    "S = requests.Session()\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def iso(date_str: str | None) -> str | None:\n",
    "    \"\"\"Turn 'YYYY-MM-DD' → 'YYYY-MM-DDT00:00:00Z' (API needs full timestamp).\"\"\"\n",
    "    return None if date_str is None else f\"{date_str}T00:00:00Z\"\n",
    "\n",
    "def fetch_revisions_api(cfg: Config):\n",
    "    assert cfg.fetch_pages, \"No pages specified\"\n",
    "    params_base = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"ids|timestamp|comment|content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "    }\n",
    "\n",
    "    for title in cfg.fetch_pages:\n",
    "        params = params_base | {\n",
    "            \"titles\": title,\n",
    "            \"rvlimit\": cfg.fetch_revs_per_page or \"max\",\n",
    "            \"rvstart\": iso(cfg.fetch_end),\n",
    "            \"rvend\":   iso(cfg.fetch_start),\n",
    "            \"rvdir\": \"newer\",\n",
    "        }\n",
    "        logger.info(f\"Fetching {title!r}\")\n",
    "        data = S.get(API, params=params, timeout=30).json()\n",
    "        if \"error\" in data:\n",
    "            logger.warning(f\"API error for {title!r}: {data['error']['code']}\")\n",
    "            continue\n",
    "        revs = data[\"query\"][\"pages\"][0].get(\"revisions\", [])\n",
    "        for r in revs:\n",
    "            yield {\n",
    "                \"page\": title,\n",
    "                \"rev_id\": r[\"revid\"],\n",
    "                \"timestamp\": r[\"timestamp\"],\n",
    "                \"wikitext\": r[\"slots\"][\"main\"][\"content\"],\n",
    "            }\n",
    "        time.sleep(cfg.fetch_delay)\n"
   ],
   "id": "88edff586f4300cd",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.583540Z",
     "start_time": "2025-05-05T19:13:52.463061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 4 – Given successive revisions of *one* page, emit only the inserted words\n",
    "# Cell 4  (edit)\n",
    "TOKEN_RX = re.compile(r\"[A-Za-z]{3,}\")   # accept 3-letter words\n",
    "\n",
    "def extract_added_text(revisions: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    revisions must be in chronological order.\n",
    "    Returns list of dicts with 'timestamp' and 'added_text'.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_lines = []\n",
    "    for rev in revisions:\n",
    "        curr_lines = rev[\"wikitext\"].splitlines()\n",
    "        diff = difflib.ndiff(prev_lines, curr_lines)\n",
    "        added_words = []\n",
    "        for line in diff:\n",
    "            if line.startswith(\"+ \"):\n",
    "                added_words.extend(TOKEN_RX.findall(line[2:].lower()))\n",
    "        if added_words:\n",
    "            out.append({\n",
    "                \"timestamp\": rev[\"timestamp\"],\n",
    "                \"added_text\": \" \".join(added_words)\n",
    "            })\n",
    "        prev_lines = curr_lines\n",
    "    return out\n"
   ],
   "id": "55b27efb05f8fa5b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.710742Z",
     "start_time": "2025-05-05T19:13:52.593307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 5 – Unifies fetch + diff + JSONL into one generator of tiny dicts\n",
    "def load_revisions(cfg: Config):\n",
    "    if cfg.fetch_pages:\n",
    "        # Group API revisions by page, then diff locally\n",
    "        buf: list[dict] = []\n",
    "        for raw_rev in fetch_revisions_api(cfg):\n",
    "            if buf and raw_rev[\"page\"] != buf[0][\"page\"]:\n",
    "                yield from extract_added_text(buf)\n",
    "                buf = []\n",
    "            buf.append(raw_rev)\n",
    "        if buf:\n",
    "            yield from extract_added_text(buf)\n",
    "    else:\n",
    "        assert cfg.file_paths, \"Provide file_paths or fetch_pages\"\n",
    "        for path in cfg.file_paths:\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    yield {\n",
    "                        \"timestamp\": obj[cfg.timestamp_field],\n",
    "                        \"added_text\": obj[cfg.text_field],\n",
    "                    }\n"
   ],
   "id": "5bea14ea780d00e4",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.842484Z",
     "start_time": "2025-05-05T19:13:52.726485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 6 – Keeps ≤ sample_size random revisions\n",
    "def sample_revs(stream, cfg: Config):\n",
    "    if cfg.sample_size_revisions is None:\n",
    "        yield from stream\n",
    "        return\n",
    "\n",
    "    random.seed(cfg.sample_seed)\n",
    "    sample, n_seen = [], 0\n",
    "    k = cfg.sample_size_revisions\n",
    "    for item in stream:\n",
    "        n_seen += 1\n",
    "        if len(sample) < k:\n",
    "            sample.append(item)\n",
    "        else:\n",
    "            j = random.randrange(n_seen)\n",
    "            if j < k:\n",
    "                sample[j] = item\n",
    "    yield from sample\n"
   ],
   "id": "7d96c87923f5d790",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:52.955295Z",
     "start_time": "2025-05-05T19:13:52.847413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 7\n",
    "def build_counts(rev_iter, cfg: Config) -> pd.DataFrame:\n",
    "    counts: dict[tuple[str, str], int] = {}\n",
    "    totals: dict[str, int] = {}\n",
    "\n",
    "    for rev in rev_iter:\n",
    "        month = rev[\"timestamp\"][:7]              # YYYY-MM\n",
    "        tokens = TOKEN_RX.findall(rev[\"added_text\"])\n",
    "        totals[month] = totals.get(month, 0) + len(tokens)\n",
    "        for w in tokens:\n",
    "            key = (w, month)\n",
    "            counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "    rows = []\n",
    "    for (w, m), c in counts.items():\n",
    "        rows.append({\"word\": w,\n",
    "                     \"month\": m,\n",
    "                     \"count\": c,\n",
    "                     \"tokens_total\": totals[m]})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"freq_obs\"] = 1_000_000 * df[\"count\"] / df[\"tokens_total\"]\n",
    "    return df\n"
   ],
   "id": "80566f14f2d5772f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:53.071420Z",
     "start_time": "2025-05-05T19:13:52.964140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 8\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "def fit_baseline(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"month_int\"] = df[\"month\"].str.replace(\"-\", \"\").astype(int)\n",
    "\n",
    "    base = df[(df[\"month\"] >= cfg.baseline_start) & (df[\"month\"] <= cfg.baseline_end)].copy()\n",
    "    if base.empty:\n",
    "        raise ValueError(\"No rows fall inside baseline window – widen fetch_start/end!\")\n",
    "\n",
    "    preds = []\n",
    "    for w, group in base.groupby(\"word\"):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        X = group[[\"month_int\"]]\n",
    "        y = group[\"freq_obs\"]\n",
    "        model = TheilSenRegressor().fit(X, y)\n",
    "        later = df[df[\"word\"] == w]\n",
    "        later[\"freq_exp\"] = model.predict(later[[\"month_int\"]])\n",
    "        preds.append(later)\n",
    "\n",
    "    return pd.concat(preds, ignore_index=True)\n"
   ],
   "id": "63821a08a80c691a",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:53.189603Z",
     "start_time": "2025-05-05T19:13:53.079289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 9\n",
    "def detect_spikes(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    post = df[df[\"month\"] > cfg.baseline_end].copy()\n",
    "    post[\"gap\"]   = post[\"freq_obs\"] - post[\"freq_exp\"]\n",
    "    post[\"ratio\"] = post[\"freq_obs\"] / post[\"freq_exp\"].replace(0, np.nan)\n",
    "    spikes = (\n",
    "        post[(post[\"gap\"] >= cfg.gap_min) & (post[\"ratio\"] >= cfg.ratio_min)]\n",
    "        .sort_values([\"gap\", \"ratio\"], ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return spikes[[\"word\", \"month\", \"gap\", \"ratio\"]]\n"
   ],
   "id": "93ec8f1e9b934361",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:13:53.304880Z",
     "start_time": "2025-05-05T19:13:53.198225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 10\n",
    "def run_pipeline(cfg: Config):\n",
    "    stream   = load_revisions(cfg)\n",
    "    stream   = sample_revs(stream, cfg)\n",
    "    counts   = build_counts(stream, cfg)\n",
    "    baseline = fit_baseline(counts, cfg)\n",
    "    spikes   = detect_spikes(baseline, cfg)\n",
    "    return baseline, spikes\n"
   ],
   "id": "78b8e94aad84cbb3",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:16:14.528317Z",
     "start_time": "2025-05-05T19:16:13.889947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 11 – Run on **tiny** live sample\n",
    "\n",
    "cfg = Config(\n",
    "    fetch_pages=[\"ChatGPT\", \"OpenAI\"],\n",
    "    fetch_start=\"2022-12-15\",\n",
    "    fetch_end=\"2024-01-07\",\n",
    "    fetch_revs_per_page=50,         # ≤ 50 per page\n",
    "    sample_size_revisions=None,     # keep all fetched\n",
    "    baseline_start=\"2022-11\",       # short baseline, just for demo\n",
    "    baseline_end=\"2022-12\",\n",
    "    gap_min=0.2, ratio_min=3        # lenient thresholds for tiny data\n",
    ")\n",
    "\n",
    "baseline_df, spike_df = run_pipeline(cfg)\n",
    "\n",
    "display(baseline_df.head())\n",
    "display(spike_df.head(20))\n"
   ],
   "id": "d8586e9e8679764a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Fetching 'ChatGPT'\n",
      "INFO Fetching 'OpenAI'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'count'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 17\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Cell 11 – Run on **tiny** live sample\u001B[39;00m\n\u001B[0;32m      6\u001B[0m cfg \u001B[38;5;241m=\u001B[39m Config(\n\u001B[0;32m      7\u001B[0m     fetch_pages\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGPT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      8\u001B[0m     fetch_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2022-12-15\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     gap_min\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, ratio_min\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m        \u001B[38;5;66;03m# lenient thresholds for tiny data\u001B[39;00m\n\u001B[0;32m     15\u001B[0m )\n\u001B[1;32m---> 17\u001B[0m baseline_df, spike_df \u001B[38;5;241m=\u001B[39m \u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m display(baseline_df\u001B[38;5;241m.\u001B[39mhead())\n\u001B[0;32m     20\u001B[0m display(spike_df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m20\u001B[39m))\n",
      "Cell \u001B[1;32mIn[44], line 5\u001B[0m, in \u001B[0;36mrun_pipeline\u001B[1;34m(cfg)\u001B[0m\n\u001B[0;32m      3\u001B[0m stream   \u001B[38;5;241m=\u001B[39m load_revisions(cfg)\n\u001B[0;32m      4\u001B[0m stream   \u001B[38;5;241m=\u001B[39m sample_revs(stream, cfg)\n\u001B[1;32m----> 5\u001B[0m counts   \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m baseline \u001B[38;5;241m=\u001B[39m fit_baseline(counts, cfg)\n\u001B[0;32m      7\u001B[0m spikes   \u001B[38;5;241m=\u001B[39m detect_spikes(baseline, cfg)\n",
      "Cell \u001B[1;32mIn[41], line 21\u001B[0m, in \u001B[0;36mbuild_counts\u001B[1;34m(rev_iter, cfg)\u001B[0m\n\u001B[0;32m     16\u001B[0m     rows\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m: w,\n\u001B[0;32m     17\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmonth\u001B[39m\u001B[38;5;124m\"\u001B[39m: m,\n\u001B[0;32m     18\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m: c,\n\u001B[0;32m     19\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens_total\u001B[39m\u001B[38;5;124m\"\u001B[39m: totals[m]})\n\u001B[0;32m     20\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(rows)\n\u001B[1;32m---> 21\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfreq_obs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1_000_000\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcount\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m/\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens_total\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m--> 418\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'count'"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:21:47.090446Z",
     "start_time": "2025-05-05T19:21:46.503937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice, tee\n",
    "# Clone the generator so we don't consume the original\n",
    "raw_stream, dbg_stream = tee(load_revisions(cfg))\n",
    "\n",
    "print(\"First 3 raw items from load_revisions:\")\n",
    "for x in islice(dbg_stream, 3):\n",
    "    print(x)\n",
    "\n",
    "# Re-assign the untouched copy back into the pipeline run\n",
    "def run_pipeline_debug(cfg):\n",
    "    stream = raw_stream                        # ← use the clone\n",
    "    stream = sample_revs(stream, cfg)\n",
    "    counts = build_counts(stream, cfg)\n",
    "    print(\"Counts shape:\", counts.shape)\n",
    "    return counts\n",
    "\n"
   ],
   "id": "3a0bc56000ebed81",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Fetching 'ChatGPT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 raw items from load_revisions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO Fetching 'OpenAI'\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T19:21:48.586341Z",
     "start_time": "2025-05-05T19:21:48.441913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "counts_df = run_pipeline_debug(cfg)\n",
    "counts_df"
   ],
   "id": "9177bc46f0049beb",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'count'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m counts_df \u001B[38;5;241m=\u001B[39m \u001B[43mrun_pipeline_debug\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m counts_df\n",
      "Cell \u001B[1;32mIn[53], line 13\u001B[0m, in \u001B[0;36mrun_pipeline_debug\u001B[1;34m(cfg)\u001B[0m\n\u001B[0;32m     11\u001B[0m stream \u001B[38;5;241m=\u001B[39m raw_stream                        \u001B[38;5;66;03m# ← use the clone\u001B[39;00m\n\u001B[0;32m     12\u001B[0m stream \u001B[38;5;241m=\u001B[39m sample_revs(stream, cfg)\n\u001B[1;32m---> 13\u001B[0m counts \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_counts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCounts shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, counts\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m counts\n",
      "Cell \u001B[1;32mIn[41], line 21\u001B[0m, in \u001B[0;36mbuild_counts\u001B[1;34m(rev_iter, cfg)\u001B[0m\n\u001B[0;32m     16\u001B[0m     rows\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m: w,\n\u001B[0;32m     17\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmonth\u001B[39m\u001B[38;5;124m\"\u001B[39m: m,\n\u001B[0;32m     18\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m: c,\n\u001B[0;32m     19\u001B[0m                  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens_total\u001B[39m\u001B[38;5;124m\"\u001B[39m: totals[m]})\n\u001B[0;32m     20\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(rows)\n\u001B[1;32m---> 21\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfreq_obs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1_000_000\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcount\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m/\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens_total\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m--> 418\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'count'"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
